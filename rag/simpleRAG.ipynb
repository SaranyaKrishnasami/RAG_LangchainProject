{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG Pipeline \n",
    "First step is to perform Data Ingestion\n",
    "    - Text document\n",
    "    - Web page scraping with Beautiful soup\n",
    "    - Pdf document\n",
    "    - Can be done for Excel / DIR files / readme file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 1 : Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TYPE 1 : Data ingestion | Text Document\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(r\"C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/policy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_documents  = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Identity-based policies â€“ Attach managed and inline policies to IAM identities (users, groups to which users belong, or roles). Identity-based policies grant permissions to an identity.\\n\\nResource-based policies â€“ Attach inline policies to resources. The most common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust policies. Resource-based policies grant permissions to the principal that is specified in the policy. Principals can be in the same account as the resource or in other accounts.\\n\\nPermissions boundaries â€“ Use a managed policy as the permissions boundary for an IAM entity (user or role). That policy defines the maximum permissions that the identity-based policies can grant to an entity, but does not grant permissions. Permissions boundaries do not define the maximum permissions that a resource-based policy can grant to an entity.\\n\\nOrganizations SCPs â€“ Use an AWS Organizations service control policy (SCP) to define the maximum permissions for account members of an organization or organizational unit (OU). SCPs limit permissions that identity-based policies or resource-based policies grant to entities (users or roles) within the account, but do not grant permissions.\\n\\nAccess control lists (ACLs) â€“ Use ACLs to control which principals in other accounts can access the resource to which the ACL is attached. ACLs are similar to resource-based policies, although they are the only policy type that does not use the JSON policy document structure. ACLs are cross-account permissions policies that grant permissions to the specified principal. ACLs cannot grant permissions to entities within the same account.\\n\\nSession policies â€“ Pass advanced session policies when you use the AWS CLI or AWS API to assume a role or a federated user. Session policies limit the permissions that the role or user's identity-based policies grant to the session. Session policies limit permissions for a created session, but do not grant permissions. For more information, see Session Policies.\", metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/policy.txt'})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TYPE 2 : Data ingestion | Web source\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(web_paths= (\"https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html\", ) , bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "    class_=(\"awsui-util-container\")    \n",
    ")))\n",
    "web_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Policies and permissions in IAMYou manage access in AWS by creating policies and attaching them to IAM identities\\n    (users, groups of users, or roles) or AWS resources. A policy is an object in AWS that, when\\n    associated with an identity or resource, defines their permissions. AWS evaluates these\\n    policies when an IAM principal (user or role) makes a request. Permissions in the policies\\n    determine whether the request is allowed or denied. Most policies are stored in AWS as JSON\\n    documents. AWS supports six types of policies: identity-based policies, resource-based\\n    policies, permissions boundaries, Organizations SCPs, ACLs, and session policies.IAM policies define permissions for an action regardless of the method that you use to\\n    perform the operation. For example, if a policy allows the GetUser action, then a user with that policy can\\n    get user information from the AWS Management Console, the AWS CLI, or the AWS API. When you create an IAM\\n    user, you can choose to allow console or programmatic access. If console access is allowed, the\\n    IAM user can sign in to the console using their sign-in credentials. If programmatic access is\\n    allowed, the user can use access keys to work with the CLI or API.\\nPolicy types\\nThe following policy types, listed in order from most frequently used to less frequently\\n      used, are available for use in AWS. For more details, see the sections below for each policy\\n      type.\\n\\n\\nIdentity-based\\n            policies – Attach managed\\n          and inline policies to IAM identities (users, groups to\\n          which users belong, or roles). Identity-based policies grant permissions to an\\n          identity.\\n\\nResource-based\\n              policies – Attach inline policies to resources. The most\\n          common examples of resource-based policies are Amazon S3 bucket policies and IAM role trust\\n          policies. Resource-based policies grant permissions to the principal that is specified in\\n          the policy. Principals can be in the same account as the resource or in other\\n          accounts.\\n\\nPermissions\\n            boundaries – Use a managed policy as the permissions boundary\\n          for an IAM entity (user or role). That policy defines the maximum permissions that the\\n          identity-based policies can grant to an entity, but does not grant permissions.\\n          Permissions boundaries do not define the maximum permissions that a resource-based policy\\n          can grant to an entity.\\n\\nOrganizations\\n            SCPs – Use an AWS Organizations service control policy (SCP) to define\\n          the maximum permissions for account members of an organization or organizational unit\\n          (OU). SCPs limit permissions that identity-based policies or resource-based policies grant\\n          to entities (users or roles) within the account, but do not grant permissions.\\n\\nAccess control lists\\n            (ACLs) – Use ACLs to control which principals in other accounts\\n          can access the resource to which the ACL is attached. ACLs are similar to resource-based\\n          policies, although they are the only policy type that does not use the JSON policy\\n          document structure. ACLs are cross-account permissions policies that grant permissions to\\n          the specified principal. ACLs cannot grant permissions to entities within the same\\n          account.\\n\\nSession\\n            policies – Pass advanced session policies when you use the\\n          AWS CLI or AWS API to assume a role or a federated user. Session policies limit the\\n          permissions that the role or user\\'s identity-based policies grant to the session. Session\\n          policies limit permissions for a created session, but do not grant permissions. For more\\n          information, see Session\\n            Policies.\\n\\nIdentity-based policies\\nIdentity-based policies are JSON permissions policy documents that control what actions\\n        an identity (users, groups of users, and roles) can perform, on which resources, and under\\n        what conditions. Identity-based policies can be further categorized:\\n\\n\\nManaged\\n              policies – Standalone identity-based policies that you can attach to\\n            multiple users, groups, and roles in your AWS account. There are two types of managed\\n            policies:\\n\\n\\nAWS managed policies – Managed\\n                policies that are created and managed by AWS.\\n\\nCustomer managed policies – Managed\\n                policies that you create and manage in your AWS account. Customer managed policies\\n                provide more precise control over your policies than AWS managed policies.\\n\\n\\nInline\\n              policies – Policies that you add directly to a single user, group,\\n            or role. Inline policies maintain a strict one-to-one relationship between a policy and\\n            an identity. They are deleted when you delete the identity.\\n\\nTo learn how to choose between managed and inline policies, see Choosing between managed\\n            policies and inline policies.\\nResource-based policies\\nResource-based policies are JSON policy documents that you attach to a resource such as\\n        an Amazon S3 bucket. These policies grant the specified principal permission to perform specific\\n        actions on that resource and defines under what conditions this applies. Resource-based\\n        policies are inline policies. There are no managed resource-based policies. \\nTo enable cross-account access, you can specify an entire account or IAM entities in\\n        another account as the principal in a resource-based policy. Adding a cross-account\\n        principal to a resource-based policy is only half of establishing the trust relationship.\\n        When the principal and the resource are in separate AWS accounts, you must also use an\\n        identity-based policy to grant the principal access to the resource. However, if a\\n        resource-based policy grants access to a principal in the same account, no additional\\n        identity-based policy is required. For step-by step instructions for granting cross-service\\n        access, see IAM tutorial: Delegate access across AWS\\n         accounts using IAM roles.\\nThe IAM service supports only one type of resource-based policy called a role\\n          trust policy, which is attached to an IAM role. An\\n        IAM role is both an identity and a resource that supports resource-based policies. For\\n        that reason, you must attach both a trust policy and an identity-based policy to an IAM\\n        role. Trust policies define which principal entities (accounts, users, roles, and federated\\n        users) can assume the role. To learn how IAM roles are different from other resource-based\\n        policies, see Cross account resource\\n            access in IAM.\\nTo see which other services support resource-based policies, see AWS services that work with\\n      IAM. To learn more about\\n        resource-based policies, see Identity-based policies and\\n         resource-based policies. \\nTo learn whether principals in accounts outside of your zone of trust (trusted organization or account) have access to assume your roles, see \\nWhat is IAM Access Analyzer?.\\nIAM permissions boundaries\\nA permissions boundary is an advanced feature in which you set the maximum permissions\\n        that an identity-based policy can grant to an IAM entity. When you set a permissions\\n        boundary for an entity, the entity can perform only the actions that are allowed by both its\\n        identity-based policies and its permissions boundaries. Resource-based policies that specify\\n        the user or role as the principal are not limited by the permissions boundary. An explicit\\n        deny in any of these policies overrides the allow. For more information about permissions\\n        boundaries, see Permissions boundaries for IAM\\n            entities.\\nService control policies (SCPs)\\nAWS Organizations is a service for grouping and centrally managing the AWS accounts that your\\n        business owns. If you enable all features in an organization, then you can apply service\\n        control policies (SCPs) to any or all of your accounts. SCPs are JSON policies that specify\\n        the maximum permissions for an organization or organizational unit (OU). The SCP limits\\n        permissions for entities in member accounts, including each AWS account root user. An explicit deny in\\n        any of these policies overrides the allow.\\nFor more information about Organizations and SCPs, see How SCPs Work in the\\n          AWS Organizations User Guide.\\nAccess control lists (ACLs)\\nAccess control lists (ACLs) are service policies that allow you to control which\\n        principals in another account can access a resource. ACLs cannot be used to control access\\n        for a principal within the same account. ACLs are similar to resource-based policies,\\n        although they are the only policy type that does not use the JSON policy document format.\\n        Amazon S3, AWS WAF, and Amazon VPC are examples of services that support ACLs. To learn more about ACLs,\\n        see Access Control List (ACL) Overview in\\n        the Amazon Simple Storage Service Developer Guide.\\nSession policies\\nSession policies are advanced policies that you pass as a parameter when you\\n        programmatically create a temporary session for a role or federated user. The permissions\\n        for a session are the intersection of the identity-based policies for the IAM entity (user\\n        or role) used to create the session and the session policies. Permissions can also come from\\n        a resource-based policy. An explicit deny in any of these policies overrides the\\n        allow.\\nYou can create role session and pass session policies programmatically using the\\n          AssumeRole, AssumeRoleWithSAML, or\\n          AssumeRoleWithWebIdentity API operations. You can pass a single JSON inline\\n        session policy document using the Policy parameter. You can use the\\n          PolicyArns parameter to specify up to 10 managed session policies. For more\\n        information about creating a role session, see Requesting temporary security credentials.\\nWhen you create a federated user session, you use the access keys of the IAM user to\\n        programmatically call the GetFederationToken API operation. You must also pass\\n        session policies. The resulting session\\'s permissions are the intersection of the\\n        identity-based policy and the session policy. For more information about creating a\\n        federated user session, see GetFederationToken—federation through a custom identity broker.\\nA resource-based policy can specify the ARN of the user or role as a principal. In that\\n        case, the permissions from the resource-based policy are added to the role or user\\'s\\n        identity-based policy before the session is created. The session policy limits the total\\n        permissions granted by the resource-based policy and the identity-based policy. The\\n        resulting session\\'s permissions are the intersection of the session policies and the\\n        resource-based policies plus the intersection of the session policies and identity-based\\n        policies.\\n\\n\\n\\nA resource-based policy can specify the ARN of the session as a principal. In that case,\\n        the permissions from the resource-based policy are added after the session is created. The\\n        resource-based policy permissions are not limited by the session policy. The resulting\\n        session has all the permissions of the resource-based policy plus the intersection of the identity-based policy and the session\\n        policy.\\n\\n\\n\\nA permissions boundary can set the maximum permissions for a user or role that is used\\n        to create a session. In that case, the resulting session\\'s permissions are the intersection\\n        of the session policy, the permissions boundary, and the identity-based policy. However, a\\n        permissions boundary does not limit permissions granted by a resource-based policy that\\n        specifies the ARN of the resulting session.\\n\\n\\n\\nPolicies and the root user\\nThe AWS account root user is affected by some policy types but not others. You cannot attach\\n      identity-based policies to the root user, and you cannot set the permissions boundary for the\\n      root user. However, you can specify the root user as the principal in a resource-based policy or an\\n      ACL. A root user is still the member of an account. If that account is a member of an\\n      organization in AWS Organizations, the root user is affected by any SCPs for the account.\\nOverview of JSON policies\\nMost policies are stored in AWS as JSON documents. Identity-based policies and policies\\n      used to set permissions boundaries are JSON policy documents that you attach to a user or\\n      role. Resource-based policies are JSON policy documents that you attach to a resource. SCPs\\n      are JSON policy documents with restricted syntax that you attach to an AWS Organizations organizational\\n      unit (OU). ACLs are also attached to a resource, but you must use a different syntax. Session\\n      policies are JSON policies that you provide when you assume a role or federated user\\n      session.\\nIt is not necessary for you to understand the JSON syntax. You can use the visual editor\\n      in the AWS Management Console to create and edit customer managed policies without ever using JSON.\\n      However, if you use inline policies for groups or complex policies, you must still create and\\n      edit those policies in the JSON editor using the console. For more information about using the\\n      visual editor, see Creating IAM policies and Editing IAM policies.\\n\\nWhen you create or edit a JSON policy, IAM can perform policy validation to help you create an effective policy. IAM identifies JSON syntax errors, while IAM Access Analyzer provides \\nadditional policy checks with recommendations to help you further refine your policies. To learn more about policy validation, see Validating IAM policies. To learn more about IAM Access Analyzer policy checks and actionable recommendations, see \\nIAM Access Analyzer policy validation.\\n\\nJSON policy document structure\\nAs illustrated in the following figure, a JSON policy document includes these\\n          elements:\\n\\n\\nOptional policy-wide information at the top of the document\\n\\nOne or more individual statements\\n\\nEach statement includes information about a single permission. If a policy includes\\n        multiple statements, AWS applies a logical OR across the statements when\\n        evaluating them. If multiple policies apply to a request, AWS applies a logical\\n          OR across all of those policies when evaluating them. \\n\\n\\n\\nThe information in a statement is contained within a series of elements.\\n\\n\\nVersion – Specify the version\\n            of the policy language that you want to use. We recommend that you use the latest\\n              2012-10-17 version. For more information, see IAM JSON policy elements:\\n        Version\\n\\nStatement – Use this main\\n            policy element as a container for the following elements. You can include more than one\\n            statement in a policy.\\n\\nSid (Optional) – Include an\\n            optional statement ID to differentiate between your statements.\\n\\nEffect – Use\\n              Allow or Deny to indicate whether the policy allows or\\n            denies access.\\n\\nPrincipal (Required in only some\\n            circumstances) – If you create a resource-based policy, you must indicate the\\n            account, user, role, or federated user to which you would like to allow or deny access.\\n            If you are creating an IAM permissions policy to attach to a user or role, you cannot\\n            include this element. The principal is implied as that user or role.\\n\\nAction – Include a list of\\n            actions that the policy allows or denies.\\n\\nResource (Required in only some\\n            circumstances) – If you create an IAM permissions policy, you must specify a\\n            list of resources to which the actions apply. If you create a resource-based policy,\\n            this element is optional. If you do not include this element, then the resource to which\\n            the action applies is the resource to which the policy is attached.\\n\\nCondition (Optional) –\\n            Specify the circumstances under which the policy grants permission.\\n\\nTo learn about these and other more advanced policy elements, see IAM JSON policy elements reference.\\n      \\nMultiple statements and multiple\\n          policies\\nIf you want to define more than one permission for an entity (user or role), you can\\n          use multiple statements in a single policy. You can also attach multiple policies. If you\\n          try to define multiple permissions in a single statement, your policy might not grant the\\n          access that you expect. We recommend that you break up policies by resource type. \\nBecause of the limited size of policies, it\\n        might be necessary to use multiple policies for more complex permissions. It\\'s also a good\\n        idea to create functional groupings of permissions in a separate customer managed policy.\\n        For example, Create one policy for IAM user management, one for self-management, and\\n        another policy for S3 bucket management. Regardless of the combination of multiple\\n        statements and multiple policies, AWS evaluates your policies the same way.\\nFor example, the following policy has three statements, each of which defines a separate\\n        set of permissions within a single account. The statements define the following:\\n\\n\\nThe first statement, with an Sid (Statement ID) of\\n              FirstStatement, lets the user with the attached policy change their own\\n            password. The Resource element in this statement is \"*\" (which\\n            means \"all resources\"). But in practice, the ChangePassword API operation\\n            (or equivalent change-password CLI command) affects only the password for\\n            the user who makes the request. \\n\\nThe second statement lets the user list all the Amazon S3 buckets in their AWS account.\\n            The Resource element in this statement is \"*\" (which means\\n            \"all resources\"). But because policies don\\'t grant access to resources in other\\n            accounts, the user can list only the buckets in their own AWS account. \\n\\nThe third statement lets the user list and retrieve any object that is in a bucket\\n            named confidential-data, but only when the user is authenticated with\\n            multi-factor authentication (MFA). The Condition element in the policy\\n            enforces the MFA authentication.\\nWhen a policy statement contains a Condition element, the statement is\\n            only in effect when the Condition element evaluates to true. In this case,\\n            the Condition evaluates to true when the user is MFA-authenticated. If the\\n            user is not MFA-authenticated, this Condition evaluates to false. In that\\n            case, the third statement in this policy does not apply and the user does not have\\n            access to the confidential-data bucket.\\n\\n{\\n  \"Version\": \"2012-10-17\",\\n  \"Statement\": [\\n    {\\n      \"Sid\": \"FirstStatement\",\\n      \"Effect\": \"Allow\",\\n      \"Action\": [\"iam:ChangePassword\"],\\n      \"Resource\": \"*\"\\n    },\\n    {\\n      \"Sid\": \"SecondStatement\",\\n      \"Effect\": \"Allow\",\\n      \"Action\": \"s3:ListAllMyBuckets\",\\n      \"Resource\": \"*\"\\n    },\\n    {\\n      \"Sid\": \"ThirdStatement\",\\n      \"Effect\": \"Allow\",\\n      \"Action\": [\\n        \"s3:List*\",\\n        \"s3:Get*\"\\n      ],\\n      \"Resource\": [\\n        \"arn:aws:s3:::confidential-data\",\\n        \"arn:aws:s3:::confidential-data/*\"\\n      ],\\n      \"Condition\": {\"Bool\": {\"aws:MultiFactorAuthPresent\": \"true\"}}\\n    }\\n  ]\\n}\\nExamples of JSON policy syntax\\nThe following identity-based policy allows the implied principal to list a single Amazon S3\\n        bucket named example_bucket: \\n{\\n  \"Version\": \"2012-10-17\",\\n  \"Statement\": {\\n    \"Effect\": \"Allow\",\\n    \"Action\": \"s3:ListBucket\",\\n    \"Resource\": \"arn:aws:s3:::example_bucket\"\\n  }\\n}\\nThe following resource-based policy can be attached to an Amazon S3 bucket. The policy allows\\n        members of a specific AWS account to perform any Amazon S3 actions in the bucket named\\n          mybucket. It allows any action that can be performed on a bucket or the\\n        objects within it. (Because the policy grants trust only to the account, individual users in\\n        the account must still be granted permissions for the specified Amazon S3 actions.) \\n{\\n  \"Version\": \"2012-10-17\",\\n  \"Statement\": [{\\n    \"Sid\": \"1\",\\n    \"Effect\": \"Allow\",\\n    \"Principal\": {\"AWS\": [\"arn:aws:iam::account-id:root\"]},\\n    \"Action\": \"s3:*\",\\n    \"Resource\": [\\n      \"arn:aws:s3:::mybucket\",\\n      \"arn:aws:s3:::mybucket/*\"\\n    ]\\n  }]\\n}\\nTo view example policies for common scenarios, see Example IAM identity-based policies. \\nGrant least privilege\\nWhen you create IAM policies, follow the standard security advice of granting\\n        least privilege, or granting only the permissions required to perform\\n        a task. Determine what users and roles need to do and then craft policies that allow them\\n        to perform only those tasks. \\nStart with a minimum set of permissions and grant additional permissions as necessary.\\n        Doing so is more secure than starting with permissions that are too lenient and then trying\\n        to tighten them later.\\nAs an alternative to least privilege, you can use AWS managed policies or policies with wildcard * permissions to get\\n        started with policies. Consider the security risk of granting your principals more\\n        permissions than they need to do their job. Monitor those principals to learn which\\n        permissions they are using. Then write least privilege policies.\\nIAM provides several options to help you refine the permissions that you grant.\\n\\n\\nUnderstand access level groupings – You can\\n                use access level groupings to understand the level of access that a policy grants. Policy actions are classified as\\n                List, Read, Write, Permissions\\n                    management, or Tagging. For example, you can choose actions from\\n                the List and Read access levels to grant read-only access to\\n                your users. To learn how to use policy summaries to understand access level permissions,\\n                see Understanding\\n      access levels in policy summaries.\\n\\nValidate your policies – You can perform\\n                policy validation using IAM Access Analyzer when you create and edit JSON policies. We\\n                recommend that you review and validate all of your existing policies. IAM Access Analyzer\\n                provides over 100 policy checks to validate your policies. It generates security\\n                warnings when a statement in your policy allows access we consider overly permissive.\\n                You can use the actionable recommendations that are provided through the security\\n                warnings as you work toward granting least privilege. To learn more about policy checks\\n                provided by IAM Access Analyzer, see IAM Access Analyzer policy\\n                    validation.\\n\\nGenerate a policy based on access activity –\\n                To help you refine the permissions that you grant, you can generate an IAM policy that\\n                is based on the access activity for an IAM entity (user or role). IAM Access Analyzer\\n                reviews your AWS CloudTrail logs and generates a policy template that contains the permissions\\n                that have been used by the entity in your specified time frame. You can use the template\\n                to create a managed policy with fine-grained permissions and then attach it to the IAM\\n                entity. That way, you grant only the permissions that the user or role needs to interact\\n                with AWS resources for your specific use case. To learn more, see Generate policies based on access\\n            activity.\\n\\nUse last accessed information – Another\\n                feature that can help with least privilege is last accessed\\n                    information. View this information on the Access\\n                        Advisor tab on the IAM console details page for an IAM user, group,\\n                role, or policy. Last accessed information also includes information about the actions\\n                that were last accessed for some services, such as Amazon EC2, IAM, Lambda, and Amazon S3. If you\\n                sign in using AWS Organizations management account credentials, you can view service last accessed\\n                information in the AWS Organizations section of the IAM console. You can\\n                also use the AWS CLI or AWS API to retrieve a report for last accessed information for\\n                entities or policies in IAM or Organizations. You can use this information to identify\\n                unnecessary permissions so that you can refine your IAM or Organizations policies to better\\n                adhere to the principle of least privilege. For more information, see Refining permissions in AWS using last\\n         accessed information.\\n\\nReview account events in AWS CloudTrail – To\\n                further reduce permissions, you can view your account\\'s events in AWS CloudTrail\\n                Event history. CloudTrail event logs include detailed event information\\n                that you can use to reduce the policy\\'s permissions. The logs include only the actions\\n                and resources that your IAM entities need. For more information, see Viewing CloudTrail Events in the CloudTrail\\n                    Console in the AWS CloudTrail User Guide.\\n\\n\\nFor more information, see the following policy topics for individual services, which provide examples of how to write\\n        policies for service-specific resources.\\n\\n\\nAuthentication and Access Control\\n                for Amazon DynamoDB in the Amazon DynamoDB Developer Guide\\n\\nUsing Bucket Policies and User\\n                Policies in the Amazon Simple Storage Service User Guide\\n\\nAccess Control List (ACL)\\n                Overview in the Amazon Simple Storage Service User Guide\\n\\n Javascript is disabled or is unavailable in your browser.To use the Amazon Web Services Documentation, Javascript must be enabled. Please refer to your browser\\'s Help pages for instructions.Document ConventionsAccess managementManaged policies and inline policies', metadata={'source': 'https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html'})]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Type 3 : Data ingestion | PDF document\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Distributional Perspective on Reinforcement Learning\\nMarc G. Bellemare* 1Will Dabney* 1R´emi Munos1\\nAbstract\\nIn this paper we argue for the fundamental impor-\\ntance of the value distribution : the distribution\\nof the random return received by a reinforcement\\nlearning agent. This is in contrast to the com-\\nmon approach to reinforcement learning which\\nmodels the expectation of this return, or value .\\nAlthough there is an established body of liter-\\nature studying the value distribution, thus far it\\nhas always been used for a speciﬁc purpose such\\nas implementing risk-aware behaviour. We begin\\nwith theoretical results in both the policy eval-\\nuation and control settings, exposing a signiﬁ-\\ncant distributional instability in the latter. We\\nthen use the distributional perspective to design\\na new algorithm which applies Bellman’s equa-\\ntion to the learning of approximate value distri-\\nbutions. We evaluate our algorithm using the\\nsuite of games from the Arcade Learning En-\\nvironment. We obtain both state-of-the-art re-\\nsults and anecdotal evidence demonstrating the\\nimportance of the value distribution in approxi-\\nmate reinforcement learning. Finally, we com-\\nbine theoretical and empirical evidence to high-\\nlight the ways in which the value distribution im-\\npacts learning in the approximate setting.\\n1. Introduction\\nOne of the major tenets of reinforcement learning states\\nthat, when not otherwise constrained in its behaviour, an\\nagent should aim to maximize its expected utility Q, or\\nvalue (Sutton & Barto, 1998). Bellman’s equation succintly\\ndescribes this value in terms of the expected reward and ex-\\npected outcome of the random transition (x,a)→(X′,A′):\\nQ(x,a) =ER(x,a) +γEQ(X′,A′).\\nIn this paper, we aim to go beyond the notion of value and\\nargue in favour of a distributional perspective on reinforce-\\n*Equal contribution1DeepMind, London, UK. Correspon-\\ndence to: Marc G. Bellemare <bellemare@google.com >.\\nProceedings of the 34thInternational Conference on Machine\\nLearning , Sydney, Australia, PMLR 70, 2017. Copyright 2017\\nby the author(s).ment learning. Speciﬁcally, the main object of our study is\\nthe random return Zwhose expectation is the value Q. This\\nrandom return is also described by a recursive equation, but\\none of a distributional nature:\\nZ(x,a)D=R(x,a) +γZ(X′,A′).\\nThedistributional Bellman equation states that the distribu-\\ntion ofZis characterized by the interaction of three random\\nvariables: the reward R, the next state-action (X′,A′), and\\nits random return Z(X′,A′). By analogy with the well-\\nknown case, we call this quantity the value distribution .\\nAlthough the distributional perspective is almost as old\\nas Bellman’s equation itself (Jaquette, 1973; Sobel, 1982;\\nWhite, 1988), in reinforcement learning it has thus far been\\nsubordinated to speciﬁc purposes: to model parametric un-\\ncertainty (Dearden et al., 1998), to design risk-sensitive al-\\ngorithms (Morimura et al., 2010b;a), or for theoretical anal-\\nysis (Azar et al., 2012; Lattimore & Hutter, 2012). By con-\\ntrast, we believe the value distribution has a central role to\\nplay in reinforcement learning.\\nContraction of the policy evaluation Bellman operator.\\nBasing ourselves on results by R ¨osler (1992) we show that,\\nfor a ﬁxed policy, the Bellman operator over value distribu-\\ntions is a contraction in a maximal form of the Wasserstein\\n(also called Kantorovich or Mallows) metric. Our partic-\\nular choice of metric matters: the same operator is not a\\ncontraction in total variation, Kullback-Leibler divergence,\\nor Kolmogorov distance.\\nInstability in the control setting. We will demonstrate an\\ninstability in the distributional version of Bellman’s opti-\\nmality equation, in contrast to the policy evaluation case.\\nSpeciﬁcally, although the optimality operator is a contrac-\\ntion in expected value (matching the usual optimality re-\\nsult), it is not a contraction in any metric over distributions.\\nThese results provide evidence in favour of learning algo-\\nrithms that model the effects of nonstationary policies.\\nBetter approximations. From an algorithmic standpoint,\\nthere are many beneﬁts to learning an approximate distribu-\\ntion rather than its approximate expectation. The distribu-\\ntional Bellman operator preserves multimodality in value\\ndistributions, which we believe leads to more stable learn-\\ning. Approximating the full distribution also mitigates the\\neffects of learning from a nonstationary policy. As a whole,arXiv:1707.06887v1  [cs.LG]  21 Jul 2017', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nwe argue that this approach makes approximate reinforce-\\nment learning signiﬁcantly better behaved.\\nWe will illustrate the practical beneﬁts of the distributional\\nperspective in the context of the Arcade Learning Environ-\\nment (Bellemare et al., 2013). By modelling the value dis-\\ntribution within a DQN agent (Mnih et al., 2015), we ob-\\ntain considerably increased performance across the gamut\\nof benchmark Atari 2600 games, and in fact achieve state-\\nof-the-art performance on a number of games. Our results\\necho those of Veness et al. (2015), who obtained extremely\\nfast learning by predicting Monte Carlo returns.\\nFrom a supervised learning perspective, learning the full\\nvalue distribution might seem obvious: why restrict our-\\nselves to the mean? The main distinction, of course, is that\\nin our setting there are no given targets. Instead, we use\\nBellman’s equation to make the learning process tractable;\\nwe must, as Sutton & Barto (1998) put it, “learn a guess\\nfrom a guess”. It is our belief that this guesswork ultimately\\ncarries more beneﬁts than costs.\\n2. Setting\\nWe consider an agent interacting with an environment in\\nthe standard fashion: at each step, the agent selects an ac-\\ntion based on its current state, to which the environment re-\\nsponds with a reward and the next state. We model this in-\\nteraction as a time-homogeneous Markov Decision Process\\n(X,A,R,P,γ ). As usual,XandAare respectively the\\nstate and action spaces, Pis the transition kernel P(·|x,a),\\nγ∈[0,1]is the discount factor, and Ris the reward func-\\ntion, which in this work we explicitly treat as a random\\nvariable. A stationary policy πmaps each state x∈X to a\\nprobability distribution over the action space A.\\n2.1. Bellman’s Equations\\nThereturnZπis the sum of discounted rewards along the\\nagent’s trajectory of interactions with the environment. The\\nvalue function Qπof a policyπdescribes the expected re-\\nturn from taking action a∈ A from statex∈ X , then\\nacting according to π:\\nQπ(x,a) :=EZπ(x,a) =E[∞∑\\nt=0γtR(xt,at)]\\n,(1)\\nxt∼P(·|xt−1,at−1),at∼π(·|xt),x0=x,a0=a.\\nFundamental to reinforcement learning is the use of Bell-\\nman’s equation (Bellman, 1957) to describe the value func-\\ntion:\\nQπ(x,a) =ER(x,a) +γE\\nP,πQπ(x′,a′).\\nIn reinforcement learning we are typically interested in act-\\ning so as to maximize the return. The most common ap-\\n\\x00P⇡ZR+P⇡Z\\x00ZP⇡(a)(b)(c)(d)T⇡Z\\x00Figure 1. A distributional Bellman operator with a deterministic\\nreward function: (a) Next state distribution under policy π, (b)\\nDiscounting shrinks the distribution towards 0, (c) The reward\\nshifts it, and (d) Projection step (Section 4).\\nproach for doing so involves the optimality equation\\nQ∗(x,a) =ER(x,a) +γEPmax\\na′∈AQ∗(x′,a′).\\nThis equation has a unique ﬁxed point Q∗, the optimal\\nvalue function, corresponding to the set of optimal policies\\nΠ∗(π∗is optimal if Ea∼π∗Q∗(x,a) = maxaQ∗(x,a)).\\nWe view value functions as vectors in RX×A, and the ex-\\npected reward function as one such vector. In this context,\\ntheBellman operatorTπandoptimality operator Tare\\nTπQ(x,a) :=ER(x,a) +γE\\nP,πQ(x′,a′) (2)\\nTQ(x,a) :=ER(x,a) +γEPmax\\na′∈AQ(x′,a′).(3)\\nThese operators are useful as they describe the expected\\nbehaviour of popular learning algorithms such as SARSA\\nand Q-Learning. In particular they are both contraction\\nmappings, and their repeated application to some initial Q0\\nconverges exponentially to QπorQ∗, respectively (Bert-\\nsekas & Tsitsiklis, 1996).\\n3. The Distributional Bellman Operators\\nIn this paper we take away the expectations inside Bell-\\nman’s equations and consider instead the full distribution\\nof the random variable Zπ. From here on, we will view Zπ\\nas a mapping from state-action pairs to distributions over\\nreturns, and call it the value distribution .\\nOur ﬁrst aim is to gain an understanding of the theoretical\\nbehaviour of the distributional analogues of the Bellman\\noperators, in particular in the less well-understood control\\nsetting. The reader strictly interested in the algorithmic\\ncontribution may choose to skip this section.\\n3.1. Distributional Equations\\nIt will sometimes be convenient to make use of the proba-\\nbility space (Ω,F,Pr). The reader unfamiliar with mea-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 1}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nsure theory may think of Ωas the space of all possible\\noutcomes of an experiment (Billingsley, 1995). We will\\nwrite∥u∥pto denote the Lpnorm of a vector u∈RXfor\\n1≤p≤∞ ; the same applies to vectors in RX×A. The\\nLpnorm of a random vector U: Ω→RX(orRX×A) is\\nthen∥U∥p:=[\\nE[\\n∥U(ω)∥p\\np]]1/p, and forp=∞we have\\n∥U∥∞=esssup∥U(ω)∥∞(we will omit the dependency\\nonω∈Ωwhenever unambiguous). We will denote the\\nc.d.f. of a random variable UbyFU(y) := Pr{U≤y},\\nand its inverse c.d.f. by F−1\\nU(q) := inf{y:FU(y)≥q}.\\nA distributional equation UD:=Vindicates that the ran-\\ndom variable Uis distributed according to the same law\\nasV. Without loss of generality, the reader can understand\\nthe two sides of a distributional equation as relating the dis-\\ntributions of two independent random variables. Distribu-\\ntional equations have been used in reinforcement learning\\nby Engel et al. (2005); Morimura et al. (2010a) among oth-\\ners, and in operations research by White (1988).\\n3.2. The Wasserstein Metric\\nThe main tool for our analysis is the Wasserstein metric dp\\nbetween cumulative distribution functions (see e.g. Bickel\\n& Freedman, 1981, where it is called the Mallows metric).\\nForF,Gtwo c.d.fs over the reals, it is deﬁned as\\ndp(F,G) := inf\\nU,V∥U−V∥p,\\nwhere the inﬁmum is taken over all pairs of random vari-\\nables (U,V)with respective cumulative distributions F\\nandG. The inﬁmum is attained by the inverse c.d.f. trans-\\nform of a random variable Uuniformly distributed on [0,1]:\\ndp(F,G) =∥F−1(U)−G−1(U)∥p.\\nForp<∞this is more explicitly written as\\ndp(F,G) =(∫1\\n0⏐⏐F−1(u)−G−1(u)⏐⏐pdu)1/p\\n.\\nGiven two random variables U,Vwith c.d.fsFU,FV, we\\nwill writedp(U,V) :=dp(FU,FV). We will ﬁnd it conve-\\nnient to conﬂate the random variables under consideration\\nwith their versions under the inf, writing\\ndp(U,V) = inf\\nU,V∥U−V∥p.\\nwhenever unambiguous; we believe the greater legibility\\njustiﬁes the technical inaccuracy. Finally, we extend this\\nmetric to vectors of random variables, such as value distri-\\nbutions, using the corresponding Lpnorm.\\nConsider a scalar aand a random variable AindependentofU,V . The metric dphas the following properties:\\ndp(aU,aV )≤|a|dp(U,V) (P1)\\ndp(A+U,A +V)≤dp(U,V) (P2)\\ndp(AU,AV )≤∥A∥pdp(U,V). (P3)\\nWe will need the following additional property, which\\nmakes no independence assumptions on its variables. Its\\nproof, and that of later results, is given in the appendix.\\nLemma 1 (Partition lemma) .LetA1,A2,... be a set of\\nrandom variables describing a partition of Ω, i.e.Ai(ω)∈\\n{0,1}and for any ωthere is exactly one AiwithAi(ω) =\\n1. LetU,V be two random variables. Then\\ndp(\\nU,V)\\n≤∑\\nidp(AiU,AiV).\\nLetZdenote the space of value distributions with bounded\\nmoments. For two value distributions Z1,Z2∈Z we will\\nmake use of a maximal form of the Wasserstein metric:\\n¯dp(Z1,Z2) := sup\\nx,adp(Z1(x,a),Z2(x,a)).\\nWe will use ¯dpto establish the convergence of the distribu-\\ntional Bellman operators.\\nLemma 2. ¯dpis a metric over value distributions.\\n3.3. Policy Evaluation\\nIn the policy evaluation setting (Sutton & Barto, 1998) we\\nare interested in the value function Vπassociated with a\\ngiven policy π. The analogue here is the value distribu-\\ntionZπ. In this section we characterize Zπand study the\\nbehaviour of the policy evaluation operator Tπ. We em-\\nphasize that Zπdescribes the intrinsic randomness of the\\nagent’s interactions with its environment, rather than some\\nmeasure of uncertainty about the environment itself.\\nWe view the reward function as a random vector R∈Z,\\nand deﬁne the transition operator Pπ:Z→Z\\nPπZ(x,a)D:=Z(X′,A′) (4)\\nX′∼P(·|x,a), A′∼π(·|X′),\\nwhere we use capital letters to emphasize the random na-\\nture of the next state-action pair (X′,A′). We deﬁne the\\ndistributional Bellman operator Tπ:Z→Z as\\nTπZ(x,a)D:=R(x,a) +γPπZ(x,a). (5)\\nWhileTπbears a surface resemblance to the usual Bell-\\nman operator (2), it is fundamentally different. In particu-\\nlar, three sources of randomness deﬁne the compound dis-\\ntributionTπZ:', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 2}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\na) The randomness in the reward R,\\nb) The randomness in the transition Pπ, and\\nc) The next-state value distribution Z(X′,A′).\\nIn particular, we make the usual assumption that these three\\nquantities are independent. In this section we will show\\nthat (5) is a contraction mapping whose unique ﬁxed point\\nis the random return Zπ.\\n3.3.1. C ONTRACTION IN ¯dp\\nConsider the process Zk+1:=TπZk, starting with some\\nZ0∈Z. We may expect the limiting expectation of {Zk}\\nto converge exponentially quickly, as usual, to Qπ. As we\\nnow show, the process converges in a stronger sense: Tπ\\nis a contraction in ¯dp, which implies that all moments also\\nconverge exponentially quickly.\\nLemma 3.Tπ:Z→Z is aγ-contraction in ¯dp.\\nUsing Lemma 3, we conclude using Banach’s ﬁxed point\\ntheorem thatTπhas a unique ﬁxed point. By inspection,\\nthis ﬁxed point must be Zπas deﬁned in (1). As we assume\\nall moments are bounded, this is sufﬁcient to conclude that\\nthe sequence{Zk}converges to Zπin¯dpfor1≤p≤∞ .\\nTo conclude, we remark that not all distributional metrics\\nare equal; for example, Chung & Sobel (1987) have shown\\nthatTπis not a contraction in total variation distance. Sim-\\nilar results can be derived for the Kullback-Leibler diver-\\ngence and the Kolmogorov distance.\\n3.3.2. C ONTRACTION IN CENTERED MOMENTS\\nObserve that d2(U,V)(and more generally, dp) relates to a\\ncouplingC(ω) :=U(ω)−V(ω), in the sense that\\nd2\\n2(U,V)≤E[(U−V)2] =V(\\nC)\\n+(\\nEC)2.\\nAs a result, we cannot directly use d2to bound the variance\\ndifference|V(TπZ(x,a))−V(Zπ(x,a))|. However,Tπ\\nis in fact a contraction in variance (Sobel, 1982, see also\\nappendix). In general, Tπis not a contraction in the pth\\ncentered moment, p >2, but the centered moments of the\\niterates{Zk}still converge exponentially quickly to those\\nofZπ; the proof extends the result of R ¨osler (1992).\\n3.4. Control\\nThus far we have considered a ﬁxed policy π, and studied\\nthe behaviour of its associated operator Tπ. We now set\\nout to understand the distributional operators of the control\\nsetting – where we seek a policy πthat maximizes value\\n– and the corresponding notion of an optimal value distri-\\nbution. As with the optimal value function, this notion is\\nintimately tied to that of an optimal policy. However, while\\nall optimal policies attain the same value Q∗, in our casea difﬁculty arises: in general there are many optimal value\\ndistributions.\\nIn this section we show that the distributional analogue\\nof the Bellman optimality operator converges, in a weak\\nsense, to the set of optimal value distributions. However,\\nthis operator is not a contraction in any metric between dis-\\ntributions, and is in general much more temperamental than\\nthe policy evaluation operators. We believe the conver-\\ngence issues we outline here are a symptom of the inherent\\ninstability of greedy updates, as highlighted by e.g. Tsitsik-\\nlis (2002) and most recently Harutyunyan et al. (2016).\\nLetΠ∗be the set of optimal policies. We begin by charac-\\nterizing what we mean by an optimal value distribution .\\nDeﬁnition 1 (Optimal value distribution) .An optimal\\nvalue distribution is the v.d. of an optimal policy. The set\\nof optimal value distributions is Z∗:={Zπ∗:π∗∈Π∗}.\\nWe emphasize that not all value distributions with expecta-\\ntionQ∗are optimal: they must match the full distribution\\nof the return under some optimal policy.\\nDeﬁnition 2. A greedy policy πforZ∈Z maximizes the\\nexpectation of Z. The set of greedy policies for Zis\\nGZ:={π:∑\\naπ(a|x)EZ(x,a) = max\\na′∈AEZ(x,a′)}.\\nRecall that the expected Bellman optimality operator Tis\\nTQ(x,a) =ER(x,a) +γEPmax\\na′∈AQ(x′,a′).(6)\\nThe maximization at x′corresponds to some greedy policy.\\nAlthough this policy is implicit in (6), we cannot ignore it\\nin the distributional setting. We will call a distributional\\nBellman optimality operator any operatorTwhich imple-\\nments a greedy selection rule, i.e.\\nTZ=TπZfor someπ∈GZ.\\nAs in the policy evaluation setting, we are interested in the\\nbehaviour of the iterates Zk+1:=TZk,Z0∈Z. Our ﬁrst\\nresult is to assert that EZkbehaves as expected.\\nLemma 4. LetZ1,Z2∈Z. Then\\n∥ETZ1−ETZ2∥∞≤γ∥EZ1−EZ2∥∞,\\nand in particular EZk→Q∗exponentially quickly.\\nBy inspecting Lemma 4, we might expect that Zkcon-\\nverges quickly in ¯dpto some ﬁxed point in Z∗. Unfor-\\ntunately, convergence is neither quick nor assured to reach\\na ﬁxed point. In fact, the best we can hope for is pointwise\\nconvergence, not even to the set Z∗but to the larger set of\\nnonstationary optimal value distributions .', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 3}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nDeﬁnition 3. A nonstationary optimal value distribution\\nZ∗∗is the value distribution corresponding to a sequence\\nof optimal policies. The set of n.o.v.d. is Z∗∗.\\nTheorem 1 (Convergence in the control setting) .LetXbe\\nmeasurable and suppose that Ais ﬁnite. Then\\nlim\\nk→∞inf\\nZ∗∗∈Z∗∗dp(Zk(x,a),Z∗∗(x,a)) = 0∀x,a.\\nIfXis ﬁnite, then Zkconverges toZ∗∗uniformly. Further-\\nmore, if there is a total ordering ≺onΠ∗, such that for any\\nZ∗∈Z∗,\\nTZ∗=TπZ∗withπ∈GZ∗, π≺π′∀π′∈GZ∗\\\\{π}.\\nThenThas a unique ﬁxed point Z∗∈Z∗.\\nComparing Theorem 1 to Lemma 4 reveals a signiﬁcant\\ndifference between the distributional framework and the\\nusual setting of expected return. While the mean of Zk\\nconverges exponentially quickly to Q∗, its distribution need\\nnot be as well-behaved! To emphasize this difference, we\\nnow provide a number of negative results concerning T.\\nProposition 1. The operatorTis not a contraction.\\nConsider the following example (Figure 2, left). There are\\ntwo states,x1andx2; a unique transition from x1tox2;\\nfromx2, actiona1yields no reward, while the optimal ac-\\ntiona2yields 1 +ϵor−1 +ϵwith equal probability. Both\\nactions are terminal. There is a unique optimal policy and\\ntherefore a unique ﬁxed point Z∗. Now consider Zas given\\nin Figure 2 (right), and its distance to Z∗:\\n¯d1(Z,Z∗) =d1(Z(x2,a2),Z∗(x2,a2)) = 2ϵ,\\nwhere we made use of the fact that Z=Z∗everywhere\\nexcept at (x2,a2). When we apply TtoZ, however, the\\ngreedy action a1is selected andTZ(x1) =Z(x2,a1). But\\nd1(TZ,TZ∗) =d1(TZ(x1),Z∗(x1))\\n=1\\n2|1−ϵ|+1\\n2|1 +ϵ|>2ϵ\\nfor a sufﬁciently small ϵ. This shows that the undiscounted\\nupdate is not a nonexpansion: ¯d1(TZ,TZ∗)>¯d1(Z,Z∗).\\nWithγ < 1, the same proof shows it is not a contraction.\\nUsing a more technically involved argument, we can extend\\nthis result to any metric which separates ZandTZ.\\nProposition 2. Not all optimality operators have a ﬁxed\\npointZ∗=TZ∗.\\nTo see this, consider the same example, now with ϵ= 0,\\nand a greedy operator Twhich breaks ties by picking a2\\nifZ(x1) = 0 , anda1otherwise. Then the sequence\\nTZ∗(x1),(T)2Z∗(x1), ... alternates between Z∗(x2,a1)\\nandZ∗(x2,a2).\\nR = 0R = /u1D700 ± 1x2x1a1a2x1x2,a1x2,a2\\nZ∗ϵ±1 0 ϵ±1\\nZ ϵ±1 0−ϵ±1\\nTZ 0 0 ϵ±1\\nFigure 2. Undiscounted two-state MDP for which the optimality\\noperatorTis not a contraction, with example. The entries that\\ncontribute to ¯d1(Z,Z∗)and¯d1(TZ,Z∗)are highlighted.\\nProposition 3. ThatThas a ﬁxed point Z∗=TZ∗is\\ninsufﬁcient to guarantee the convergence of {Zk}toZ∗.\\nTheorem 1 paints a rather bleak picture of the control set-\\nting. It remains to be seen whether the dynamical eccen-\\ntricies highlighted here actually arise in practice. One open\\nquestion is whether theoretically more stable behaviour can\\nbe derived using stochastic policies, for example from con-\\nservative policy iteration (Kakade & Langford, 2002).\\n4. Approximate Distributional Learning\\nIn this section we propose an algorithm based on the dis-\\ntributional Bellman optimality operator. In particular, this\\nwill require choosing an approximating distribution. Al-\\nthough the Gaussian case has previously been considered\\n(Morimura et al., 2010a; Tamar et al., 2016), to the best of\\nour knowledge we are the ﬁrst to use a rich class of para-\\nmetric distributions.\\n4.1. Parametric Distribution\\nWe will model the value distribution using a discrete distri-\\nbution parametrized by N∈NandVMIN,VMAX∈R, and\\nwhose support is the set of atoms {zi=VMIN+i△z: 0≤\\ni < N},△z:=VMAX−VMIN\\nN−1. In a sense, these atoms are the\\n“canonical returns” of our distribution. The atom probabil-\\nities are given by a parametric model θ:X×A→ RN\\nZθ(x,a) =ziw.p.pi(x,a) :=eθi(x,a)\\n∑\\njeθj(x,a).\\nThe discrete distribution has the advantages of being highly\\nexpressive and computationally friendly (see e.g. Van den\\nOord et al., 2016).\\n4.2. Projected Bellman Update\\nUsing a discrete distribution poses a problem: the Bell-\\nman updateTZθand our parametrization Zθalmost al-\\nways have disjoint supports. From the analysis of Section\\n3 it would seem natural to minimize the Wasserstein met-\\nric (viewed as a loss) between TZθandZθ, which is also', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 4}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nconveniently robust to discrepancies in support. However,\\na second issue prevents this: in practice we are typically\\nrestricted to learning from sample transitions, which is not\\npossible under the Wasserstein loss (see Prop. 5 and toy\\nresults in the appendix).\\nInstead, we project the sample Bellman update ˆTZθonto\\nthe support of Zθ(Figure 1, Algorithm 1), effectively re-\\nducing the Bellman update to multiclass classiﬁcation. Let\\nπbe the greedy policy w.r.t. EZθ. Given a sample transi-\\ntion(x,a,r,x′), we compute the Bellman update ˆTzj:=\\nr+γzjfor each atom zj, then distribute its probability\\npj(x′,π(x′))to the immediate neighbours of ˆTzj. Theith\\ncomponent of the projected update ΦˆTZθ(x,a)is\\n(ΦˆTZθ(x,a))i=N−1∑\\nj=0[\\n1−|[ˆTzj]VMAX\\nVMIN−zi|\\n△z]1\\n0pj(x′,π(x′)),\\n(7)\\nwhere [·]b\\nabounds its argument in the range [a,b].1As is\\nusual, we view the next-state distribution as parametrized\\nby a ﬁxed parameter ˜θ. The sample loss Lx,a(θ)is the\\ncross-entropy term of the KL divergence\\nDKL(ΦˆTZ˜θ(x,a)∥Zθ(x,a)),\\nwhich is readily minimized e.g. using gradient descent. We\\ncall this choice of distribution and loss the categorical al-\\ngorithm . WhenN= 2, a simple one-parameter alternative\\nisΦˆTZθ(x,a) := [ E[ˆTZθ(x,a)]−VMIN)/△z]1\\n0;we call\\nthis the Bernoulli algorithm . We note that, while these al-\\ngorithms appear unrelated to the Wasserstein metric, recent\\nwork (Bellemare et al., 2017) hints at a deeper connection.\\nAlgorithm 1 Categorical Algorithm\\ninput A transition xt,at,rt,xt+1,γt∈[0,1]\\nQ(xt+1,a) :=∑\\nizipi(xt+1,a)\\na∗←arg maxaQ(xt+1,a)\\nmi= 0, i∈0,...,N−1\\nforj∈0,...,N−1do\\n# Compute the projection of ˆTzjonto the support{zi}\\nˆTzj←[rt+γtzj]VMAX\\nVMIN\\nbj←(ˆTzj−VMIN)/∆z#bj∈[0,N−1]\\nl←⌊bj⌋,u←⌈bj⌉\\n# Distribute probability of ˆTzj\\nml←ml+pj(xt+1,a∗)(u−bj)\\nmu←mu+pj(xt+1,a∗)(bj−l)\\nend for\\noutput−∑\\nimilogpi(xt,at)# Cross-entropy loss\\n5. Evaluation on Atari 2600 Games\\nTo understand the approach in a complex setting, we ap-\\nplied the categorical algorithm to games from the Ar-\\n1Algorithm 1 computes this projection in time linear in N.cade Learning Environment (ALE; Bellemare et al., 2013).\\nWhile the ALE is deterministic, stochasticity does occur in\\na number of guises: 1) from state aliasing, 2) learning from\\na nonstationary policy, and 3) from approximation errors.\\nWe used ﬁve training games (Fig 3) and 52 testing games.\\nFor our study, we use the DQN architecture (Mnih et al.,\\n2015), but output the atom probabilities pi(x,a)instead\\nof action-values, and chose VMAX=−VMIN= 10 from\\npreliminary experiments over the training games. We call\\nthe resulting architecture Categorical DQN . We replace the\\nsquared loss (r+γQ(x′,π(x′))−Q(x,a))2byLx,a(θ)\\nand train the network to minimize this loss.2As in DQN,\\nwe use a simple ϵ-greedy policy over the expected action-\\nvalues; we leave as future work the many ways in which an\\nagent could select actions on the basis of the full distribu-\\ntion. The rest of our training regime matches Mnih et al.’s,\\nincluding the use of a target network for ˜θ.\\nFigure 4 illustrates the typical value distributions we ob-\\nserved in our experiments. In this example, three actions\\n(those including the button press) lead to the agent releas-\\ning its laser too early and eventually losing the game. The\\ncorresponding distributions reﬂect this: they assign a sig-\\nniﬁcant probability to 0 (the terminal value). The safe\\nactions have similar distributions ( LEFT , which tracks the\\ninvaders’ movement, is slightly favoured). This example\\nhelps explain why our approach is so successful: the dis-\\ntributional update keeps separated the low-value, “losing”\\nevent from the high-value, “survival” event, rather than av-\\nerage them into one (unrealizable) expectation.3\\nOne surprising fact is that the distributions are not concen-\\ntrated on one or two values, in spite of the ALE’s determin-\\nism, but are often close to Gaussians. We believe this is due\\nto our discretizing the diffusion process induced by γ.\\n5.1. Varying the Number of Atoms\\nWe began by studying our algorithm’s performance on the\\ntraining games in relation to the number of atoms (Figure\\n3). For this experiment, we set ϵ= 0.05. From the data, it\\nis clear that using too few atoms can lead to poor behaviour,\\nand that more always increases performance; this is not im-\\nmediately obvious as we may have expected to saturate the\\nnetwork capacity. The difference in performance between\\nthe 51-atom version and DQN is particularly striking: the\\nlatter is outperformed in all ﬁve games, and in S EAQUEST\\nwe attain state-of-the-art performance. As an additional\\npoint of the comparison, the single-parameter Bernoulli al-\\ngorithm performs better than DQN in 3 games out of 5, and\\nis most notably more robust in A STERIX .\\n2ForN= 51 , our TensorFlow implementation trains at\\nroughly 75% of DQN’s speed.\\n3Video: http://youtu.be/yFBwyPuO2Vg .', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 5}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nASTERIX\\nQ*BERTBREAKOUTPONGSEAQUESTCategorical DQN5 returns11 re tur ns21 returns51 returnsDQNBernoulliAverage ScoreTraining Frames (millions)Dueling Arch.\\nFigure 3. Categorical DQN: Varying number of atoms in the discrete distribution. Scores are moving averages over 5 million frames.\\nReturnProbabilityRightLeftRight+LaserLeft+LaserLaserNoop\\nFigure 4. Learned value distribution during an episode of S PACE\\nINVADERS . Different actions are shaded different colours. Re-\\nturns below 0 (which do not occur in S PACE INVADERS ) are not\\nshown here as the agent assigns virtually no probability to them.\\nOne interesting outcome of this experiment was to ﬁnd\\nout that our method does pick up on stochasticity. P ONG\\nexhibits intrinsic randomness: the exact timing of the re-\\nward depends on internal registers and is truly unobserv-\\nable. We see this clearly reﬂected in the agent’s prediction\\n(Figure 5): over ﬁve consecutive frames, the value distribu-\\ntion shows two modes indicating the agent’s belief that it\\nhas yet to receive a reward. Interestingly, since the agent’s\\nstate does not include past rewards, it cannot even extin-\\nguish the prediction after receiving the reward, explaining\\nthe relative proportions of the modes.\\n5.2. State-of-the-Art Results\\nThe performance of the 51-atom agent (from here onwards,\\nC51) on the training games, presented in the last section, is\\nparticularly remarkable given that it involved none of the\\nother algorithmic ideas present in state-of-the-art agents.\\nWe next asked whether incorporating the most common\\nhyperparameter choice, namely a smaller training ϵ, could\\nlead to even better results. Speciﬁcally, we set ϵ= 0.01\\n(instead of 0.05); furthermore, every 1 million frames, weevaluate our agent’s performance with ϵ= 0.001.\\nWe compare our algorithm to DQN ( ϵ= 0.01), Double\\nDQN (van Hasselt et al., 2016), the Dueling architecture\\n(Wang et al., 2016), and Prioritized Replay (Schaul et al.,\\n2016), comparing the best evaluation score achieved during\\ntraining. We see that C51 signiﬁcantly outperforms these\\nother algorithms (Figures 6 and 7). In fact, C51 surpasses\\nthe current state-of-the-art by a large margin in a number of\\ngames, most notably S EAQUEST . One particularly striking\\nfact is the algorithm’s good performance on sparse reward\\ngames, for example V ENTURE and P RIVATE EYE. This\\nsuggests that value distributions are better able to propa-\\ngate rarely occurring events. Full results are provided in\\nthe appendix.\\nWe also include in the appendix (Figure 12) a compari-\\nson, averaged over 3 seeds, showing the number of games\\nin which C51’s training performance outperforms fully-\\ntrained DQN and human players. These results continue\\nto show dramatic improvements, and are more representa-\\ntive of an agent’s average performance. Within 50 million\\nframes, C51 has outperformed a fully trained DQN agent\\non 45 out of 57 games. This suggests that the full 200 mil-\\nlion training frames, and its ensuing computational cost,\\nare unnecessary for evaluating reinforcement learning al-\\ngorithms within the ALE.\\nThe most recent version of the ALE contains a stochastic\\nexecution mechanism designed to ward against trajectory\\noverﬁtting.Speciﬁcally, on each frame the environment re-\\njects the agent’s selected action with probability p= 0.25.\\nAlthough DQN is mostly robust to stochastic execution,\\nthere are a few games in which its performance is reduced.\\nOn a score scale normalized with respect to the random\\nand DQN agents, C51 obtains mean and median score im-\\nprovements of 126% and21.5%respectively, conﬁrming\\nthe beneﬁts of C51 beyond the deterministic setting.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 6}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nFigure 5. Intrinsic stochasticity in P ONG.\\nMean Median>H.B.>DQN\\nDQN 228% 79% 24 0\\nDDQN 307% 118% 33 43\\nDUEL. 373% 151% 37 50\\nPRIOR . 434% 124% 39 48\\nPR. DUEL.592% 172% 39 44\\nC51 701% 178% 40 50\\nUNREAL†880% 250% - -\\nFigure 6. Mean and median scores across 57 Atari games, mea-\\nsured as percentages of human baseline (H.B., Nair et al., 2015).\\nFigure 7. Percentage improvement, per-game, of C51 over Dou-\\nble DQN, computed using van Hasselt et al.’s method.\\n6. Discussion\\nIn this work we sought a more complete picture of rein-\\nforcement learning, one that involves value distributions.\\nWe found that learning value distributions is a powerful no-\\ntion that allows us to surpass most gains previously made\\non Atari 2600, without further algorithmic adjustments.\\n6.1. Why does learning a distribution matter?\\nIt is surprising that, when we use a policy which aims to\\nmaximize expected return, we should see any difference\\nin performance. The distinction we wish to make is that\\nlearning distributions matters in the presence of approxi-\\nmation . We now outline some possible reasons.\\nReduced chattering. Our results from Section 3.4 high-\\nlighted a signiﬁcant instability in the Bellman optimal-\\nity operator. When combined with function approxima-\\ntion, this instability may prevent the policy from converg-\\ning, what Gordon (1995) called chattering . We believe\\nthe gradient-based categorical algorithm is able to mitigate\\nthese effects by effectively averaging the different distri-\\n†The UNREAL results are not altogether comparable, as\\nthey were generated in the asynchronous setting with per-game\\nhyperparameter tuning (Jaderberg et al., 2017).butions, similar to conservative policy iteration (Kakade &\\nLangford, 2002). While the chattering persists, it is inte-\\ngrated to the approximate solution.\\nState aliasing. Even in a deterministic environment, state\\naliasing may result in effective stochasticity. McCallum\\n(1995), for example, showed the importance of coupling\\nrepresentation learning with policy learning in partially ob-\\nservable domains. We saw an example of state aliasing in\\nPONG, where the agent could not exactly predict the re-\\nward timing. Again, by explicitly modelling the resulting\\ndistribution we provide a more stable learning target.\\nA richer set of predictions. A recurring theme in artiﬁcial\\nintelligence is the idea of an agent learning from a mul-\\ntitude of predictions (Caruana 1997; Utgoff & Stracuzzi\\n2002; Sutton et al. 2011; Jaderberg et al. 2017). The dis-\\ntributional approach naturally provides us with a rich set\\nof auxiliary predictions, namely: the probability that the\\nreturn will take on a particular value. Unlike previously\\nproposed approaches, however, the accuracy of these pre-\\ndictions is tightly coupled with the agent’s performance.\\nFramework for inductive bias. The distributional per-\\nspective on reinforcement learning allows a more natural\\nframework within which we can impose assumptions about\\nthe domain or the learning problem itself. In this work we\\nused distributions with support bounded in [VMIN,VMAX].\\nTreating this support as a hyperparameter allows us to\\nchange the optimization problem by treating all extremal\\nreturns (e.g. greater than VMAX) as equivalent. Surprisingly,\\na similar value clipping in DQN signiﬁcantly degrades per-\\nformance in most games. To take another example: in-\\nterpreting the discount factor γas a proper probability, as\\nsome authors have argued, leads to a different algorithm.\\nWell-behaved optimization. It is well-accepted that the\\nKL divergence between categorical distributions is a rea-\\nsonably easy loss to minimize. This may explain some of\\nour empirical performance. Yet early experiments with al-\\nternative losses, such as KL divergence between continu-\\nous densities, were not fruitful, in part because the KL di-\\nvergence is insensitive to the values of its outcomes. A\\ncloser minimization of the Wasserstein metric should yield\\neven better results than what we presented here.\\nIn closing, we believe our results highlight the need to ac-\\ncount for distribution in the design, theoretical or other-\\nwise, of algorithms.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 7}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nAcknowledgements\\nThe authors acknowledge the important role played by their\\ncolleagues at DeepMind throughout the development of\\nthis work. Special thanks to Yee Whye Teh, Alex Graves,\\nJoel Veness, Guillaume Desjardins, Tom Schaul, David\\nSilver, Andre Barreto, Max Jaderberg, Mohammad Azar,\\nGeorg Ostrovski, Bernardo Avila Pires, Olivier Pietquin,\\nAudrunas Gruslys, Tom Stepleton, Aaron van den Oord;\\nand particularly Chris Maddison for his comprehensive re-\\nview of an earlier draft. Thanks also to Marek Petrik for\\npointers to the relevant literature, and Mark Rowland for\\nﬁne-tuning details in the ﬁnal version.\\nErratum\\nThe camera-ready copy of this paper incorrectly reported a\\nmean score of 1010% for C51. The corrected ﬁgure stands\\nat 701%, which remains higher than the other comparable\\nbaselines. The median score remains unchanged at 178%.\\nThe error was due to evaluation episodes in one game (At-\\nlantis) lasting over 30 minutes; in comparison, the other\\nresults presented here cap episodes at 30 minutes, as is\\nstandard. The previously reported score on Atlantis was\\n3.7 million; our 30-minute score is 841,075, which we be-\\nlieve is close to the achievable maximum in this time frame.\\nCapping at 30 minutes brings our human-normalized score\\non Atlantis from 22824% to a mere (!) 5199%, unfortu-\\nnately enough to noticeably affect the mean score, whose\\nsensitivity to outliers is well-documented.\\nReferences\\nAzar, Mohammad Gheshlaghi, Munos, R ´emi, and Kappen,\\nHilbert. On the sample complexity of reinforcement learning\\nwith a generative model. In Proceedings of the International\\nConference on Machine Learning , 2012.\\nBellemare, Marc G, Naddaf, Yavar, Veness, Joel, and Bowling,\\nMichael. The arcade learning environment: An evaluation plat-\\nform for general agents. Journal of Artiﬁcial Intelligence Re-\\nsearch , 47:253–279, 2013.\\nBellemare, Marc G., Danihelka, Ivo, Dabney, Will, Mo-\\nhamed, Shakir, Lakshminarayanan, Balaji, Hoyer, Stephan,\\nand Munos, R ´emi. The cramer distance as a solution to biased\\nwasserstein gradients. arXiv , 2017.\\nBellman, Richard E. Dynamic programming . Princeton Univer-\\nsity Press, Princeton, NJ, 1957.\\nBertsekas, Dimitri P. and Tsitsiklis, John N. Neuro-Dynamic Pro-\\ngramming . Athena Scientiﬁc, 1996.\\nBickel, Peter J. and Freedman, David A. Some asymptotic the-\\nory for the bootstrap. The Annals of Statistics , pp. 1196–1217,\\n1981.\\nBillingsley, Patrick. Probability and measure . John Wiley &\\nSons, 1995.Caruana, Rich. Multitask learning. Machine Learning , 28(1):\\n41–75, 1997.\\nChung, Kun-Jen and Sobel, Matthew J. Discounted mdps: Distri-\\nbution functions and exponential utility maximization. SIAM\\nJournal on Control and Optimization , 25(1):49–62, 1987.\\nDearden, Richard, Friedman, Nir, and Russell, Stuart. Bayesian\\nQ-learning. In Proceedings of the National Conference on Ar-\\ntiﬁcial Intelligence , 1998.\\nEngel, Yaakov, Mannor, Shie, and Meir, Ron. Reinforcement\\nlearning with gaussian processes. In Proceedings of the In-\\nternational Conference on Machine Learning , 2005.\\nGeist, Matthieu and Pietquin, Olivier. Kalman temporal differ-\\nences. Journal of Artiﬁcial Intelligence Research , 39:483–532,\\n2010.\\nGordon, Geoffrey. Stable function approximation in dynamic pro-\\ngramming. In Proceedings of the Twelfth International Confer-\\nence on Machine Learning , 1995.\\nHarutyunyan, Anna, Bellemare, Marc G., Stepleton, Tom, and\\nMunos, R ´emi. Q(λ) with off-policy corrections. In Proceed-\\nings of the Conference on Algorithmic Learning Theory , 2016.\\nHoffman, Matthew D., de Freitas, Nando, Doucet, Arnaud, and\\nPeters, Jan. An expectation maximization algorithm for con-\\ntinuous markov decision processes with arbitrary reward. In\\nProceedings of the International Conference on Artiﬁcial In-\\ntelligence and Statistics , 2009.\\nJaderberg, Max, Mnih, V olodymyr, Czarnecki, Wojciech Marian,\\nSchaul, Tom, Leibo, Joel Z, Silver, David, and Kavukcuoglu,\\nKoray. Reinforcement learning with unsupervised auxiliary\\ntasks. Proceedings of the International Conference on Learn-\\ning Representations , 2017.\\nJaquette, Stratton C. Markov decision processes with a new opti-\\nmality criterion: Discrete time. The Annals of Statistics , 1(3):\\n496–505, 1973.\\nKakade, Sham and Langford, John. Approximately optimal ap-\\nproximate reinforcement learning. In Proceedings of the Inter-\\nnational Conference on Machine Learning , 2002.\\nKingma, Diederik and Ba, Jimmy. Adam: A method for stochas-\\ntic optimization. Proceedings of the International Conference\\non Learning Representations , 2015.\\nLattimore, Tor and Hutter, Marcus. PAC bounds for discounted\\nMDPs. In Proceedings of the Conference on Algorithmic\\nLearning Theory , 2012.\\nMannor, Shie and Tsitsiklis, John N. Mean-variance optimization\\nin markov decision processes. 2011.\\nMcCallum, Andrew K. Reinforcement learning with selective per-\\nception and hidden state . PhD thesis, University of Rochester,\\n1995.\\nMnih, V olodymyr, Kavukcuoglu, Koray, Silver, David, Rusu, An-\\ndrei A, Veness, Joel, Bellemare, Marc G, Graves, Alex, Ried-\\nmiller, Martin, Fidjeland, Andreas K, Ostrovski, Georg, et al.\\nHuman-level control through deep reinforcement learning. Na-\\nture, 518(7540):529–533, 2015.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 8}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nMorimura, Tetsuro, Hachiya, Hirotaka, Sugiyama, Masashi,\\nTanaka, Toshiyuki, and Kashima, Hisashi. Parametric return\\ndensity estimation for reinforcement learning. In Proceed-\\nings of the Conference on Uncertainty in Artiﬁcial Intelligence ,\\n2010a.\\nMorimura, Tetsuro, Sugiyama, Masashi, Kashima, Hisashi,\\nHachiya, Hirotaka, and Tanaka, Toshiyuki. Nonparametric re-\\nturn distribution approximation for reinforcement learning. In\\nProceedings of the 27th International Conference on Machine\\nLearning (ICML-10) , pp. 799–806, 2010b.\\nNair, Arun, Srinivasan, Praveen, Blackwell, Sam, Alcicek,\\nCagdas, Fearon, Rory, De Maria, Alessandro, Panneershelvam,\\nVedavyas, Suleyman, Mustafa, Beattie, Charles, and Petersen,\\nStig et al. Massively parallel methods for deep reinforcement\\nlearning. In ICML Workshop on Deep Learning , 2015.\\nPrashanth, LA and Ghavamzadeh, Mohammad. Actor-critic algo-\\nrithms for risk-sensitive mdps. In Advances in Neural Informa-\\ntion Processing Systems , 2013.\\nPuterman, Martin L. Markov Decision Processes: Discrete\\nstochastic dynamic programming . John Wiley & Sons, Inc.,\\n1994.\\nR¨osler, Uwe. A ﬁxed point theorem for distributions. Stochastic\\nProcesses and their Applications , 42(2):195–214, 1992.\\nSchaul, Tom, Quan, John, Antonoglou, Ioannis, and Silver,\\nDavid. Prioritized experience replay. In Proceedings of the\\nInternational Conference on Learning Representations , 2016.\\nSobel, Matthew J. The variance of discounted markov decision\\nprocesses. Journal of Applied Probability , 19(04):794–802,\\n1982.\\nSutton, Richard S. and Barto, Andrew G. Reinforcement learning:\\nAn introduction . MIT Press, 1998.\\nSutton, R.S., Modayil, J., Delp, M., Degris, T., Pilarski, P.M.,\\nWhite, A., and Precup, D. Horde: A scalable real-time archi-\\ntecture for learning knowledge from unsupervised sensorimo-\\ntor interaction. In Proceedings of the International Conference\\non Autonomous Agents and Multiagents Systems , 2011.\\nTamar, Aviv, Di Castro, Dotan, and Mannor, Shie. Learning the\\nvariance of the reward-to-go. Journal of Machine Learning\\nResearch , 17(13):1–36, 2016.\\nTieleman, Tijmen and Hinton, Geoffrey. Lecture 6.5-rmsprop:\\nDivide the gradient by a running average of its recent magni-\\ntude. COURSERA: Neural networks for machine learning , 4\\n(2), 2012.\\nToussaint, Marc and Storkey, Amos. Probabilistic inference for\\nsolving discrete and continuous state markov decision pro-\\ncesses. In Proceedings of the International Conference on Ma-\\nchine Learning , 2006.\\nTsitsiklis, John N. On the convergence of optimistic policy itera-\\ntion. Journal of Machine Learning Research , 3:59–72, 2002.\\nUtgoff, Paul E. and Stracuzzi, David J. Many-layered learning.\\nNeural Computation , 14(10):2497–2529, 2002.\\nVan den Oord, Aaron, Kalchbrenner, Nal, and Kavukcuoglu, Ko-\\nray. Pixel recurrent neural networks. In Proceedings of the\\nInternational Conference on Machine Learning , 2016.van Hasselt, Hado, Guez, Arthur, and Silver, David. Deep rein-\\nforcement learning with double Q-learning. In Proceedings of\\nthe AAAI Conference on Artiﬁcial Intelligence , 2016.\\nVeness, Joel, Bellemare, Marc G., Hutter, Marcus, Chua, Alvin,\\nand Desjardins, Guillaume. Compress and control. In Proceed-\\nings of the AAAI Conference on Artiﬁcial Intelligence , 2015.\\nWang, Tao, Lizotte, Daniel, Bowling, Michael, and Schuurmans,\\nDale. Dual representations for dynamic programming. Journal\\nof Machine Learning Research , pp. 1–29, 2008.\\nWang, Ziyu, Schaul, Tom, Hessel, Matteo, Hasselt, Hado van,\\nLanctot, Marc, and de Freitas, Nando. Dueling network archi-\\ntectures for deep reinforcement learning. In Proceedings of the\\nInternational Conference on Machine Learning , 2016.\\nWhite, D. J. Mean, variance, and probabilistic criteria in ﬁnite\\nmarkov decision processes: a review. Journal of Optimization\\nTheory and Applications , 56(1):1–29, 1988.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 9}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nA. Related Work\\nTo the best of our knowledge, the work closest to ours are\\ntwo papers (Morimura et al., 2010b;a) studying the distri-\\nbutional Bellman equation from the perspective of its cu-\\nmulative distribution functions. The authors propose both\\nparametric and nonparametric solutions to learn distribu-\\ntions for risk-sensitive reinforcement learning. They also\\nprovide some theoretical analysis for the policy evaluation\\nsetting, including a consistency result in the nonparamet-\\nric case. By contrast, we also analyze the control setting,\\nand emphasize the use of the distributional equations to im-\\nprove approximate reinforcement learning.\\nThe variance of the return has been extensively stud-\\nied in the risk-sensitive setting. Of note, Tamar et al.\\n(2016) analyze the use of linear function approximation\\nto learn this variance for policy evaluation, and Prashanth\\n& Ghavamzadeh (2013) estimate the return variance in the\\ndesign of a risk-sensitive actor-critic algorithm. Mannor\\n& Tsitsiklis (2011) provides negative results regarding the\\ncomputation of a variance-constrained solution to the opti-\\nmal control problem.\\nThe distributional formulation also arises when modelling\\nuncertainty. Dearden et al. (1998) considered a Gaussian\\napproximation to the value distribution, and modelled the\\nuncertainty over the parameters of this approximation us-\\ning a Normal-Gamma prior. Engel et al. (2005) leveraged\\nthe distributional Bellman equation to deﬁne a Gaussian\\nprocess over the unknown value function. More recently,\\nGeist & Pietquin (2010) proposed an alternative solution to\\nthe same problem based on unscented Kalman ﬁlters. We\\nbelieve much of the analysis we provide here, which deals\\nwith the intrinsic randomness of the environment, can also\\nbe applied to modelling uncertainty.\\nOur work here is based on a number of foundational re-\\nsults, in particular concerning alternative optimality crite-\\nria. Early on, Jaquette (1973) showed that a moment opti-\\nmality criterion, which imposes a total ordering on distri-\\nbutions, is achievable and deﬁnes a stationary optimal pol-\\nicy, echoing the second part of Theorem 1. Sobel (1982)\\nis usually cited as the ﬁrst reference to Bellman equations\\nfor the higher moments (but not the distribution) of the re-\\nturn. Chung & Sobel (1987) provides results concerning\\nthe convergence of the distributional Bellman operator in\\ntotal variation distance. White (1988) studies “nonstandard\\nMDP criteria” from the perspective of optimizing the state-\\naction pair occupancy.\\nA number of probabilistic frameworks for reinforcement\\nlearning have been proposed in recent years. The plan-\\nning as inference approach (Toussaint & Storkey, 2006;\\nHoffman et al., 2009) embeds the return into a graphical\\nmodel, and applies probabilistic inference to determine thesequence of actions leading to maximal expected reward.\\nWang et al. (2008) considered the dual formulation of re-\\ninforcement learning, where one optimizes the stationary\\ndistribution subject to constraints given by the transition\\nfunction (Puterman, 1994), in particular its relationship to\\nlinear approximation. Related to this dual is the Compress\\nand Control algorithm Veness et al. (2015), which describes\\na value function by learning a return distribution using den-\\nsity models. One of the aims of this work was to address\\nthe question left open by their work of whether one could\\nbe design a practical distributional algorithm based on the\\nBellman equation, rather than Monte Carlo estimation.\\nB. Proofs\\nLemma 1 (Partition lemma) .LetA1,A2,... be a set of\\nrandom variables describing a partition of Ω, i.e.Ai(ω)∈\\n{0,1}and for any ωthere is exactly one AiwithAi(ω) =\\n1. LetU,V be two random variables. Then\\ndp(\\nU,V)\\n≤∑\\nidp(AiU,AiV).\\nProof. We will give the proof for p <∞, noting that the\\nsame applies to p=∞. LetYiD:=AiUandZiD:=AiV,\\nrespectively. First note that\\ndp\\np(AiU,AiV) = inf\\nYi,ZiE[\\n|Yi−Zi|p]\\n= inf\\nYi,ZiE[\\nE[\\n|Yi−Zi|p|Ai]]\\n.\\nNow,|AiU−AiV|p= 0wheneverAi= 0. It follows that\\nwe can choose Yi,Ziso that also|Yi−Zi|p= 0whenever\\nAi= 0, without increasing the expected norm. Hence\\ndp\\np(AiU,AiV) =\\ninf\\nYi,ZiPr{Ai= 1}E[\\n|Yi−Zi|p|Ai= 1]\\n.(8)\\nNext, we claim that\\ninf\\nU,V∑\\niPr{Ai= 1}E[⏐⏐AiU−AiV⏐⏐p|Ai= 1]\\n(9)\\n≤inf\\nY1,Y2,...\\nZ1,Z2,...∑\\niPr{Ai= 1}E[\\n|Yi−Zi⏐⏐p|Ai= 1]\\n.\\nSpeciﬁcally, the left-hand side of the equation is an inﬁ-\\nmum over all r.v.’s whose cumulative distributions are FU\\nandFV, respectively, while the right-hand side is an in-\\nﬁmum over sequences of r.v’s Y1,Y2,... andZ1,Z2,...\\nwhose cumulative distributions are FAiU,FAiV, respec-\\ntively. To prove this upper bound, consider the c.d.f. of\\nU:\\nFU(y) = Pr{U≤y}\\n=∑\\niPr{Ai= 1}Pr{U≤y|Ai= 1}\\n=∑\\niPr{Ai= 1}Pr{AiU≤y|Ai= 1}.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 10}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nHence the distribution FUis equivalent, in an almost sure\\nsense, to one that ﬁrst picks an element Aiof the partition,\\nthen picks a value for Uconditional on the choice Ai. On\\nthe other hand, the c.d.f. of YiD=AiUis\\nFAiU(y) = Pr{Ai= 1}Pr{AiU≤y|Ai= 1}\\n+ Pr{Ai= 0}Pr{AiU≤y|Ai= 0}\\n= Pr{Ai= 1}Pr{AiU≤y|Ai= 1}\\n+ Pr{Ai= 0}I[y≥0].\\nThus the right-hand side inﬁmum in (9) has the additional\\nconstraint that it must preserve the conditional c.d.fs, in\\nparticular when y≥0. Put another way, instead of hav-\\ning the freedom to completely reorder the mapping U:\\nΩ→R, we can only reorder it within each element of the\\npartition. We now write\\ndp\\np(U,V) = inf\\nU,V∥U−V∥p\\n= inf\\nU,VE[\\n|U−V|p]\\n(a)= inf\\nU,V∑\\niPr{Ai= 1}E[\\n|U−V|p|Ai= 1]\\n= inf\\nU,V∑\\niPr{Ai= 1}E[\\n|AiU−AiV|p|Ai= 1]\\n,\\nwhere (a) follows because A1,A2,... is a partition. Using\\n(9), this implies\\ndp\\np(U,V)\\n= inf\\nU,V∑\\niPr{Ai= 1}E[⏐⏐AiU−AiV⏐⏐p|Ai= 1]\\n≤inf\\nY1,Y2,...\\nZ1,Z2,...∑\\niPr{Ai= 1}E[⏐⏐Yi−Zi⏐⏐p|Ai= 1]\\n(b)=∑\\niinf\\nYi,ZiPr{Ai= 1}E[⏐⏐Yi−Zi⏐⏐p|Ai= 1]\\n(c)=∑\\nidp(AiU,AiV),\\nbecause in (b) the individual components of the sum are\\nindependently minimized; and (c) from (8).\\nLemma 2. ¯dpis a metric over value distributions.\\nProof. The only nontrivial property is the triangle inequal-\\nity. For any value distribution Y∈Z, write\\n¯dp(Z1,Z2) = sup\\nx,adp(Z1(x,a),Z2(x,a))\\n(a)\\n≤sup\\nx,a[dp(Z1(x,a),Y(x,a)) +dp(Y(x,a),Z2(x,a))]\\n≤sup\\nx,adp(Z1(x,a),Y(x,a)) + sup\\nx,adp(Y(x,a),Z2(x,a))\\n=¯dp(Z1,Y) +¯dp(Y,Z 2),\\nwhere in (a) we used the triangle inequality for dp.Lemma 3.Tπ:Z→Z is aγ-contraction in ¯dp.\\nProof. ConsiderZ1,Z2∈Z. By deﬁnition,\\n¯dp(TπZ1,TπZ2) = sup\\nx,adp(TπZ1(x,a),TπZ2(x,a)).\\n(10)\\nBy the properties of dp, we have\\ndp(TπZ1(x,a),TπZ2(x,a))\\n=dp(R(x,a) +γPπZ1(x,a),R(x,a) +γPπZ2(x,a))\\n≤γdp(PπZ1(x,a),PπZ2(x,a))\\n≤γsup\\nx′,a′dp(Z1(x′,a′),Z2(x′,a′)),\\nwhere the last line follows from the deﬁnition of Pπ(see\\n(4)). Combining with (10) we obtain\\n¯dp(TπZ1,TπZ2) = sup\\nx,adp(TπZ1(x,a),TπZ2(x,a))\\n≤γsup\\nx′,a′dp(Z1(x′,a′),Z2(x′,a′))\\n=γ¯dp(Z1,Z2).\\nProposition 1 (Sobel, 1982) .Consider two value distri-\\nbutionsZ1,Z2∈Z , and write V(Zi)to be the vector of\\nvariances of Zi. Then\\n∥ETπZ1−ETπZ2∥∞≤γ∥EZ1−EZ2∥∞, and\\n∥V(TπZ1)−V(TπZ2)∥∞≤γ2∥VZ1−VZ2∥∞.\\nProof. The ﬁrst statement is standard, and its proof follows\\nfromETπZ=TπEZ, where the second Tπdenotes the\\nusual operator over value functions. Now, by independence\\nofRandPπZi:\\nV(TπZi(x,a)) =V(\\nR(x,a) +γPπZi(x,a))\\n=V(R(x,a)) +γ2V(PπZi(x,a)).\\nAnd now\\n∥V(TπZ1)−V(TπZ2)∥∞\\n= sup\\nx,a⏐⏐V(TπZ1(x,a))−V(TπZ2(x,a))⏐⏐\\n= sup\\nx,aγ2⏐⏐[V(PπZ1(x,a))−V(PπZ2(x,a))]⏐⏐\\n= sup\\nx,aγ2⏐⏐E[V(Z1(X′,A′))−V(Z2(X′,A′))]⏐⏐\\n≤sup\\nx′,a′γ2⏐⏐V(Z1(x′,a′))−V(Z2(x′,a′))⏐⏐\\n≤γ2∥VZ1−VZ2∥∞.\\nLemma 4. LetZ1,Z2∈Z. Then\\n∥ETZ1−ETZ2∥∞≤γ∥EZ1−EZ2∥∞,\\nand in particular EZk→Q∗exponentially quickly.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 11}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nProof. The proof follows by linearity of expectation. Write\\nTDfor the distributional operator and TEfor the usual op-\\nerator. Then\\n∥ETDZ1−ETDZ2∥∞=∥TEEZ1−TEEZ2∥∞\\n≤γ∥Z1−Z2∥∞.\\nTheorem 1 (Convergence in the control setting) .Let\\nZk:=TZk−1withZ0∈Z . LetXbe measurable and\\nsuppose thatAis ﬁnite. Then\\nlim\\nk→∞inf\\nZ∗∗∈Z∗∗dp(Zk(x,a),Z∗∗(x,a)) = 0∀x,a.\\nIfXis ﬁnite, then Zkconverges toZ∗∗uniformly. Further-\\nmore, if there is a total ordering ≺onΠ∗, such that for any\\nZ∗∈Z∗,\\nTZ∗=TπZ∗withπ∈GZ∗, π≺π′∀π′∈GZ∗\\\\{π},\\nthenThas a unique ﬁxed point Z∗∈Z∗.\\nThe gist of the proof of Theorem 1 consists in showing that\\nfor every state x, there is a time kafter which the greedy\\npolicy w.r.t. Qkis mostly optimal. To clearly expose the\\nsteps involved, we will ﬁrst assume a unique (and there-\\nfore deterministic) optimal policy π∗, and later return to\\nthe general case; we will denote the optimal action at xby\\nπ∗(x). For notational convenience, we will write Qk:=\\nEZkandGk:=GZk. LetB:= 2 supZ∈Z∥Z∥∞<∞\\nand letϵk:=γkB. We ﬁrst deﬁne the set of states Xk⊆X\\nwhose values must be sufﬁciently close to Q∗at timek:\\nXk:={\\nx:Q∗(x,π∗(x))−max\\na̸=π∗(x)Q∗(x,a)>2ϵk}\\n.\\n(11)\\nIndeed, by Lemma 4, we know that after kiterations\\n|Qk(x,a)−Q∗(x,a)|≤γk|Q0(x,a)−Q∗(x,a)|≤ϵk.\\nForx∈X, writea∗:=π∗(x). For anya∈A, we deduce\\nthat\\nQk(x,a∗)−Qk(x,a)≥Q∗(x,a∗)−Q∗(x,a)−2ϵk.\\nIt follows that if x∈Xk, then alsoQk(x,a∗)>Qk(x,a′)\\nfor alla′̸=π∗(x): for these states, the greedy policy\\nπk(x) := arg maxaQk(x,a)corresponds to the optimal\\npolicyπ∗.\\nLemma 5. For eachx∈ X there exists a ksuch\\nthat, for all k′≥k,x∈ Xk′, and in particular\\narg maxaQk(x,a) =π∗(x).\\nProof. BecauseAis ﬁnite, the gap\\n∆(x) :=Q∗(x,π∗(x))−max\\na̸=π∗(x)Q∗(x,a)is attained for some strictly positive ∆(x)>0. By deﬁni-\\ntion, there exists a ksuch that\\nϵk=γkB <∆(x)\\n2,\\nand hence every x∈X must eventually be in Xk.\\nThis lemma allows us to guarantee the existence of an\\niterationkafter which sufﬁciently many states are well-\\nbehaved, in the sense that the greedy policy at those states\\nchooses the optimal action. We will call these states\\n“solved”. We in fact require not only these states to be\\nsolved, but also most of their successors, and most of the\\nsuccessors of those, and so on. We formalize this notion as\\nfollows: ﬁx some δ > 0, letXk,0:=Xk, and deﬁne for\\ni>0the set\\nXk,i:={\\nx:x∈Xk,P(Xk−1,i−1|x,π∗(x))≥1−δ}\\n,\\nAs the following lemma shows, any xis eventually con-\\ntained in the recursively-deﬁned sets Xk,i, for anyi.\\nLemma 6. For anyi∈Nand anyx∈X, there exists a k\\nsuch that for all k′≥k,x∈Xk′,i.\\nProof. Fixiand let us suppose that Xk,i↑X. By Lemma\\n5, this is true for i= 0. We infer that for any probability\\nmeasurePonX,P(Xk,i)→P(X) = 1 . In particular, for\\na givenx∈Xk, this implies that\\nP(Xk,i|x,π∗(x))→P(X|x,π∗(x)) = 1.\\nTherefore, for any x, there exists a time after which it is\\nand remains a member of Xk,i+1, the set of states for which\\nP(Xk−1,i|x,π∗(x))≥1−δ. We conclude that Xk,i+1↑\\nXalso. The statement follows by induction.\\nProof of Theorem 1. The proof is similar to policy\\niteration-type results, but requires more care in dealing\\nwith the metric and the possibly inﬁnite state space.\\nWe will write Wk(x) :=Zk(x,πk(x)), deﬁneW∗\\nsimilarly and with some overload of notation write\\nTWk(x) :=Wk+1(x) =TZk(x,πk+1(x)). Finally, let\\nSk\\ni(x) :=I[x∈Xk,i]and¯Sk\\ni(x) = 1−Sk\\ni(x).\\nFixi >0andx∈Xk+1,i+1⊆Xk. We begin by using\\nLemma 1 to separate the transition from xinto a solved\\nterm and an unsolved term:\\nPπkWk(x) =Sk\\niWk(X′) +¯Sk\\niWk(X′),\\nwhereX′is the random successor from taking action\\nπk(x) :=π∗(x), and we write Sk\\ni=Sk\\ni(X′),¯Sk\\ni=\\n¯Sk\\ni(X′)to ease the notation. Similarly,\\nPπkW∗(x) =Sk\\niW∗(X′) +¯Sk\\niW∗(X′).', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 12}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nNow\\ndp(Wk+1(x),W∗(x)) =dp(TWk(x),TW∗(x))\\n(a)\\n≤γdp(PπkWk(x),Pπ∗W∗(x))\\n(b)\\n≤γdp(Sk\\niWk(X′),Sk\\niW∗(X′))\\n+γdp(¯Sk\\niWk(X′),¯Sk\\niW∗(X′)), (12)\\nwhere in (a)we used Properties P1 and P2 of the Wasser-\\nstein metric, and in (b) we separate states for which πk=\\nπ∗from the rest using Lemma 1 ( {Sk\\ni,¯Sk\\ni}form a parti-\\ntion of Ω). Letδi:= Pr{X′/∈Xk,i}=E{¯Sk\\ni(X′)}=\\n∥¯Sk\\ni(X′)∥p. From property P3 of the Wasserstein metric,\\nwe have\\ndp(¯Sk\\niWk(X′),¯Sk\\niW∗(X′))\\n≤sup\\nx′dp(¯Sk\\ni(X′)Wk(x′),¯Sk\\ni(X′)W∗(x′))\\n≤∥¯Sk\\ni(X′)∥psup\\nx′dp(Wk(x′),W∗(x′))\\n≤δisup\\nx′dp(Wk(x′),W∗(x′))\\n≤δiB.\\nRecall thatB <∞is the largest attainable ∥Z∥∞. Since\\nalsoδi< δ by our choice of x∈Xk+1,i+1, we can upper\\nbound the second term in (12) by γδB. This yields\\ndp(Wk+1(x),W∗(x))≤\\nγdp(Sk\\niWk(X′),Sk\\niW∗(X′)) +γδB.\\nBy induction on i >0, we conclude that for x∈Xk+i,i\\nand some random state X′′isteps forward,\\ndp(Wk+i(x),W∗(x))≤\\nγidp(Sk\\n0Wk(X′′),Sk\\n0W∗(X′′)) +δB\\n1−γ\\n≤γiB+δB\\n1−γ.\\nHence for any x∈X,ϵ>0, we can take δ,i, and ﬁnally k\\nlarge enough to make dp(Wk(x),W∗(x))< ϵ. The proof\\nthen extends to Zk(x,a)by considering one additional ap-\\nplication ofT.\\nWe now consider the more general case where there are\\nmultiple optimal policies. We expand the deﬁnition of Xk,i\\nas follows:\\nXk,i:={\\nx∈Xk:∀π∗∈Π∗,E\\na∗∼π∗(x)P(Xk−1,i−1|x,a∗)≥1−δ}\\n,\\nBecause there are ﬁnitely many actions, Lemma 6 also\\nholds for this new deﬁnition. As before, take x∈Xk,i, but\\nnow consider the sequence of greedy policies πk,πk−1,...\\nselected by successive applications of T, and write\\nT¯πk:=TπkTπk−1···Tπk−i+1,such that\\nZk+1=T¯πkZk−i+1.\\nNow denote byZ∗∗the set of nonstationary optimal poli-\\ncies. If we take any Z∗∈Z∗, we deduce that\\ninf\\nZ∗∗∈Z∗∗dp(T¯πkZ∗(x,a),Z∗∗(x,a))≤δB\\n1−γ,\\nsinceZ∗corresponds to some optimal policy π∗and¯πkis\\noptimal along most of the trajectories from (x,a). In effect,\\nT¯πkZ∗is close to the value distribution of the nonstation-\\nary optimal policy ¯πkπ∗. Now for this Z∗,\\ninf\\nZ∗∗dp(Zk(x,a),Z∗∗(x,a))\\n≤dp(Zk(x,a),T¯πkZ∗(x,a))\\n+ inf\\nZ∗∗dp(T¯πkZ∗(x,a),Z∗∗(x,a))\\n≤dp(T¯πkZk−i+1(x,a),T¯πkZ∗(x,a)) +δB\\n1−γ\\n≤γiB+2δB\\n1−γ,\\nusing the same argument as before with the newly-deﬁned\\nXk,i. It follows that\\ninf\\nZ∗∗∈Z∗∗dp(Zk(x,a),Z∗∗(x,a))→0.\\nWhenXis ﬁnite, there exists a ﬁxed kafter whichXk=\\nX. The uniform convergence result then follows.\\nTo prove the uniqueness of the ﬁxed point Z∗whenTse-\\nlects its actions according to the ordering ≺, we note that\\nfor any optimal value distribution Z∗, its set of greedy poli-\\ncies is Π∗. Denote by π∗the policy coming ﬁrst in the or-\\ndering over Π∗. ThenT=Tπ∗, which has a unique ﬁxed\\npoint (Section 3.3).\\nProposition 4. ThatThas a ﬁxed point Z∗=TZ∗is\\ninsufﬁcient to guarantee the convergence of {Zk}toZ∗.\\nWe provide here a sketch of the result. Consider a single\\nstatex1with two actions, a1anda2(Figure 8). The ﬁrst\\naction yields a reward of 1/2, while the other either yields\\n0or1with equal probability, and both actions are optimal.\\nNow takeγ= 1/2and writeR0,R1,... for the received\\nrewards. Consider a stochastic policy that takes action a2\\nwith probability p. Forp= 0, the return is\\nZp=0=1\\n1−γ1\\n2= 1.\\nForp= 1, on the other hand, the return is random and is\\ngiven by the following fractional number (in binary):\\nZp=1=R0.R1R2R3···.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 13}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nR = 1/2R = 0 or 1x1a1a2\\nFigure 8. A simple example illustrating the effect of a nonstation-\\nary policy on the value distribution.\\nAs a result,Zp=1is uniformly distributed between 0and2!\\nIn fact, note that\\nZp=0= 0.11111···= 1.\\nFor some intermediary value of p, we obtain a different\\nprobability of the different digits, but always putting some\\nprobability mass on all returns in [0,2].\\nNow suppose we follow the nonstationary policy that takes\\na1on the ﬁrst step, then a2from there on. By inspec-\\ntion, the return will be uniformly distributed on the interval\\n[1/2,3/2], which does not correspond to the return under\\nany value of p. But now we may imagine an operator T\\nwhich alternates between a1anda2depending on the ex-\\nact value distribution it is applied to, which would in turn\\nconverge to a nonstationary optimal value distribution.\\nLemma 7 (Sample Wasserstein distance) .Let{Pi}be a\\ncollection of random variables, I∈Na random index\\nindependent from{Pi}, and consider the mixture random\\nvariableP=PI. For any random variable Qindependent\\nofI,\\ndp(P,Q)≤E\\ni∼Idp(Pi,Q),\\nand in general the inequality is strict and\\n∇Qdp(PI,Q)̸=E\\ni∼I∇Qdp(Pi,Q).\\nProof. We prove this using Lemma 1. Let Ai:=I[I=i].\\nWe write\\ndp(P,Q) =dp(PI,Q)\\n=dp(∑\\niAiPi,∑\\niAiQ)\\n≤∑\\nidp(AiPi,AiQ)\\n≤∑\\niPr{I=i}dp(Pi,Q)\\n=EIdP(Pi,Q).\\nwhere in the penultimate line we used the independence of\\nIfromPiandQto appeal to property P3 of the Wasserstein\\nmetric.\\nTo show that the bound is in general strict, consider the\\nmixture distribution depicted in Figure 9. We will simplyconsider the d1metric between this distribution Pand an-\\nother distribution Q. The ﬁrst distribution is\\nP={0w.p.1/2\\n1w.p.1/2.\\nIn this example, i∈{1,2},P1= 0, andP2= 1. Now\\nconsider the distribution with the same support but that puts\\nprobabilitypon0:\\nQ={0w.p.p\\n1w.p.1−p.\\nThe distance between PandQis\\nd1(P,Q) =|p−1\\n2|.\\nThis isd1(P,Q) =1\\n2forp∈{0,1}, and strictly less than\\n1\\n2for any other values of p. On the other hand, the corre-\\nsponding expected distance (after sampling an outcome x1\\norx2with equal probability) is\\nEId1(Pi,Q) =1\\n2p+1\\n2(1−p) =1\\n2.\\nHenced1(P,Q)<EId1(Pi,Q)forp∈(0,1). This shows\\nthat the bound is in general strict. By inspection, it is clear\\nthat the two gradients are different.\\nR = 0R = 1xx1x2½½\\nFigure 9. Example MDP in which the expected sample Wasser-\\nstein distance is greater than the Wasserstein distance.\\nProposition 5. Fix some next-state distribution Zand pol-\\nicyπ. Consider a parametric value distribution Zθ, and\\nand deﬁne the Wasserstein loss\\nLW(θ) :=dp(Zθ(x,a),R(x,a) +γZ(X′,π(X′))).\\nLetr∼R(x,a)andx′∼P(·|x,a)and consider the\\nsample loss\\nLW(θ,r,x′) :=dp(Zθ(x,a),r+γZ(x′,π(x′)).\\nIts expectation is an upper bound on the loss LW:\\nLW(θ)≤E\\nR,PLW(θ,r,x′),\\nin general with strict inequality.\\nThe result follows directly from the previous lemma.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 14}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\n# AtomsWassersteinCategoricalMonte-Carlo TargetStochastic Bellman TargetWassersteinCategoricald1(Z⇡,Z✓)\\nReturnFZ(a)(b)\\nFigure 10. (a) Wasserstein distance between ground truth distribution Zπand approximating distributions Zθ. Varying number of atoms\\nin approximation, training target, and loss function. (b) Approximate cumulative distributions for ﬁve representative states in CliffWalk.\\nC. Algorithmic Details\\nWhile our training regime closely follows that of DQN\\n(Mnih et al., 2015), we use Adam (Kingma & Ba, 2015)\\ninstead of RMSProp (Tieleman & Hinton, 2012) for gra-\\ndient rescaling. We also performed some hyperparam-\\neter tuning for our ﬁnal results. Speciﬁcally, we eval-\\nuated two hyperparameters over our ﬁve training games\\nand choose the values that performed best. The hyperpa-\\nrameter values we considered were VMAX∈ {3,10,100}\\nandϵadam∈ {1/L,0.1/L,0.01/L,0.001/L,0.0001/L},\\nwhereL= 32 is the minibatch size. We found VMAX= 10\\nandϵadam = 0.01/Lperformed best. We used the same\\nstep-size value as DQN ( α= 0.00025 ).\\nPseudo-code for the categorical algorithm is given in Algo-\\nrithm 1. We apply the Bellman update to each atom sepa-\\nrately, and then project it into the two nearest atoms in the\\noriginal support. Transitions to a terminal state are handled\\nwithγt= 0.\\nD. Comparison of Sampled Wasserstein Loss\\nand Categorical Projection\\nLemma 3 proves that for a ﬁxed policy πthe distributional\\nBellman operator is a γ-contraction in ¯dp, and therefore\\nthatTπwill converge in distribution to the true distribution\\nof returnsZπ. In this section, we empirically validate these\\nresults on the CliffWalk domain shown in Figure 11. The\\ndynamics of the problem match those given by Sutton &\\nBarto (1998). We also study the convergence of the distri-\\nbutional Bellman operator under the sampled Wasserstein\\nloss and the categorical projection (Equation 7) while fol-\\nThe CliﬀSGsafe pathoptimal pathr = -1r = -100Figure 11. CliffWalk Environment (Sutton & Barto, 1998).\\nlowing a policy that tries to take the safe path but has a 10%\\nchance of taking another action uniformly at random.\\nWe compute a ground-truth distribution of returns Zπusing\\n10000 Monte-Carlo (MC) rollouts from each state. We then\\nperform two experiments, approximating the value distri-\\nbution at each state with our discrete distributions.\\nIn the ﬁrst experiment, we perform supervised learning us-\\ning either the Wasserstein loss or categorical projection\\n(Equation 7) with cross-entropy loss. We use Zπas the\\nsupervised target and perform 5000 sweeps over all states\\nto ensure both approaches have converged. In the second\\nexperiment, we use the same loss functions, but the training\\ntarget comes from the one-step distributional Bellman op-\\nerator with sampled transitions. We use VMIN=−100and\\nVMAX=−1.4For the sample updates we perform 10 times\\nas many sweeps over the state space. Fundamentally, these\\nexperiments investigate how well the two training regimes\\n4Because there is a small probability of larger negative returns,\\nsome approximation error is unavoidable. However, this effect is\\nrelatively negligible in our experiments.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 15}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\n(minimizing the Wasserstein or categorical loss) minimize\\nthe Wasserstein metric under both ideal (supervised target)\\nand practical (sampled one-step Bellman target) conditions.\\nIn Figure 10a we show the ﬁnal Wasserstein distance\\nd1(Zπ,Zθ)between the learned distributions and the\\nground-truth distribution as we vary the number of atoms.\\nThe graph shows that the categorical algorithm does indeed\\nminimize the Wasserstein metric in both the supervised and\\nsample Bellman setting. It also highlights that minimizing\\nthe Wasserstein loss with stochastic gradient descent is in\\ngeneral ﬂawed, conﬁrming the intuition given by Propo-\\nsition 5. In repeat experiments the process converged to\\ndifferent values of d1(Zπ,Zθ), suggesting the presence of\\nlocal minima (more prevalent with fewer atoms).\\nFigure 10 provides additional insight into why the sampled\\nWasserstein distance may perform poorly. Here, we see the\\ncumulative densities for the approximations learned under\\nthese two losses for ﬁve different states along the safe path\\nin CliffWalk. The Wasserstein has converged to a ﬁxed-\\npoint distribution, but not one that captures the true (Monte\\nCarlo) distribution very well. By comparison, the categor-\\nical algorithm captures the variance of the true distribution\\nmuch more accurately.\\nE. Supplemental Videos and Results\\nIn Figure 13 we provide links to supplemental videos show-\\ning the C51 agent during training on various Atari 2600\\ngames. Figure 12 shows the relative performance of C51\\nover the course of training. Figure 14 provides a table\\nof evaluation results, comparing C51 to other state-of-the-\\nart agents. Figures 15–18 depict particularly interesting\\nframes.\\n# Games SuperiorTraining Frames (millions)C51 vs. DQNC51 vs. HUMANDQN vs. HUMAN\\nFigure 12. Number of Atari games where an agent’s training per-\\nformance is greater than a baseline (fully trained DQN & human).\\nError bands give standard deviations, and averages are over num-\\nber of games.GAMES VIDEO URL\\nFreeway http://youtu.be/97578n9kFIk\\nPong http://youtu.be/vIz5P6s80qA\\nQ*Bert http://youtu.be/v-RbNX4uETw\\nSeaquest http://youtu.be/d1yz4PNFUjI\\nSpace Invaders http://youtu.be/yFBwyPuO2Vg\\nFigure 13. Supplemental videos of C51 during training.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 16}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nGAMES RANDOM HUMAN DQN DDQN DUEL PRIOR .DUEL . C51\\nAlien 227.8 7,127.7 1,620.0 3,747.7 4,461.4 3,941.0 3,166\\nAmidar 5.8 1,719.5 978.0 1,793.3 2,354.5 2,296.8 1,735\\nAssault 222.4 742.0 4,280.4 5,393.2 4,621.0 11,477.0 7,203\\nAsterix 210.0 8,503.3 4,359.0 17,356.5 28,188.0 375,080.0 406,211\\nAsteroids 719.1 47,388.7 1,364.5 734.7 2,837.7 1,192.7 1,516\\nAtlantis 12,850.0 29,028.1 279,987.0 106,056.0 382,572.0 395,762.0 841,075\\nBank Heist 14.2 753.1 455.0 1,030.6 1,611.9 1,503.1 976\\nBattle Zone 2,360.0 37,187.5 29,900.0 31,700.0 37,150.0 35,520.0 28,742\\nBeam Rider 363.9 16,926.5 8,627.5 13,772.8 12,164.0 30,276.5 14,074\\nBerzerk 123.7 2,630.4 585.6 1,225.4 1,472.6 3,409.0 1,645\\nBowling 23.1 160.7 50.4 68.1 65.5 46.7 81.8\\nBoxing 0.1 12.1 88.0 91.6 99.4 98.9 97.8\\nBreakout 1.7 30.5 385.5 418.5 345.3 366.0 748\\nCentipede 2,090.9 12,017.0 4,657.7 5,409.4 7,561.4 7,687.5 9,646\\nChopper Command 811.0 7,387.8 6,126.0 5,809.0 11,215.0 13,185.0 15,600\\nCrazy Climber 10,780.5 35,829.4 110,763.0 117,282.0 143,570.0 162,224.0 179,877\\nDefender 2,874.5 18,688.9 23,633.0 35,338.5 42,214.0 41,324.5 47,092\\nDemon Attack 152.1 1,971.0 12,149.4 58,044.2 60,813.3 72,878.6 130,955\\nDouble Dunk -18.6 -16.4 -6.6 -5.5 0.1 -12.5 2.5\\nEnduro 0.0 860.5 729.0 1,211.8 2,258.2 2,306.4 3,454\\nFishing Derby -91.7 -38.7 -4.9 15.5 46.4 41.3 8.9\\nFreeway 0.0 29.6 30.8 33.3 0.0 33.0 33.9\\nFrostbite 65.2 4,334.7 797.4 1,683.3 4,672.8 7,413.0 3,965\\nGopher 257.6 2,412.5 8,777.4 14,840.8 15,718.4 104,368.2 33,641\\nGravitar 173.0 3,351.4 473.0 412.0 588.0 238.0 440\\nH.E.R.O. 1,027.0 30,826.4 20,437.8 20,130.2 20,818.2 21,036.5 38,874\\nIce Hockey -11.2 0.9 -1.9 -2.7 0.5 -0.4 -3.5\\nJames Bond 29.0 302.8 768.5 1,358.0 1,312.5 812.0 1,909\\nKangaroo 52.0 3,035.0 7,259.0 12,992.0 14,854.0 1,792.0 12,853\\nKrull 1,598.0 2,665.5 8,422.3 7,920.5 11,451.9 10,374.4 9,735\\nKung-Fu Master 258.5 22,736.3 26,059.0 29,710.0 34,294.0 48,375.0 48,192\\nMontezuma’s Revenge 0.0 4,753.3 0.0 0.0 0.0 0.0 0.0\\nMs. Pac-Man 307.3 6,951.6 3,085.6 2,711.4 6,283.5 3,327.3 3,415\\nName This Game 2,292.3 8,049.0 8,207.8 10,616.0 11,971.1 15,572.5 12,542\\nPhoenix 761.4 7,242.6 8,485.2 12,252.5 23,092.2 70,324.3 17,490\\nPitfall! -229.4 6,463.7 -286.1 -29.9 0.0 0.0 0.0\\nPong -20.7 14.6 19.5 20.9 21.0 20.9 20.9\\nPrivate Eye 24.9 69,571.3 146.7 129.7 103.0 206.0 15,095\\nQ*Bert 163.9 13,455.0 13,117.3 15,088.5 19,220.3 18,760.3 23,784\\nRiver Raid 1,338.5 17,118.0 7,377.6 14,884.5 21,162.6 20,607.6 17,322\\nRoad Runner 11.5 7,845.0 39,544.0 44,127.0 69,524.0 62,151.0 55,839\\nRobotank 2.2 11.9 63.9 65.1 65.3 27.5 52.3\\nSeaquest 68.4 42,054.7 5,860.6 16,452.7 50,254.2 931.6 266,434\\nSkiing -17,098.1 -4,336.9 -13,062.3 -9,021.8 -8,857.4 -19,949.9 -13,901\\nSolaris 1,236.3 12,326.7 3,482.8 3,067.8 2,250.8 133.4 8,342\\nSpace Invaders 148.0 1,668.7 1,692.3 2,525.5 6,427.3 15,311.5 5,747\\nStar Gunner 664.0 10,250.0 54,282.0 60,142.0 89,238.0 125,117.0 49,095\\nSurround -10.0 6.5 -5.6 -2.9 4.4 1.2 6.8\\nTennis -23.8 -8.3 12.2 -22.8 5.1 0.0 23.1\\nTime Pilot 3,568.0 5,229.2 4,870.0 8,339.0 11,666.0 7,553.0 8,329\\nTutankham 11.4 167.6 68.1 218.4 211.4 245.9 280\\nUp and Down 533.4 11,693.2 9,989.9 22,972.2 44,939.6 33,879.1 15,612\\nVenture 0.0 1,187.5 163.0 98.0 497.0 48.0 1,520\\nVideo Pinball 16,256.9 17,667.9 196,760.4 309,941.9 98,209.5 479,197.0 949,604\\nWizard Of Wor 563.5 4,756.5 2,704.0 7,492.0 7,855.0 12,352.0 9,300\\nYars’ Revenge 3,092.9 54,576.9 18,098.9 11,712.6 49,622.1 69,618.1 35,050\\nZaxxon 32.5 9,173.3 5,363.0 10,163.0 12,944.0 13,886.0 10,513\\nFigure 14. Raw scores across all games, starting with 30 no-op actions. Reference values from Wang et al. (2016).', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 17}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nFigure 15. FREEWAY : Agent differentiates action-value distributions under pressure.\\nFigure 16. Q*B ERT: Top, left and right: Predicting which actions are unrecoverably fatal. Bottom-Left: Value distribution shows steep\\nconsequences for wrong actions. Bottom-Right: The agent has made a huge mistake.\\nFigure 17. SEAQUEST : Left: Bimodal distribution. Middle: Might hit the ﬁsh. Right: Deﬁnitely going to hit the ﬁsh.\\nFigure 18. SPACE INVADERS : Top-Left: Multi-modal distribution with high uncertainty. Top-Right: Subsequent frame, a more certain\\ndemise. Bottom-Left: Clear difference between actions. Bottom-Middle: Uncertain survival. Bottom-Right: Certain success.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 18})]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 2 : Data Transformation ( Convert into chunks to fit into model context window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents = text_splitter.split_documents(pdf_documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Distributional Perspective on Reinforcement Learning\\nMarc G. Bellemare* 1Will Dabney* 1R´emi Munos1\\nAbstract\\nIn this paper we argue for the fundamental impor-\\ntance of the value distribution : the distribution\\nof the random return received by a reinforcement\\nlearning agent. This is in contrast to the com-\\nmon approach to reinforcement learning which\\nmodels the expectation of this return, or value .\\nAlthough there is an established body of liter-\\nature studying the value distribution, thus far it\\nhas always been used for a speciﬁc purpose such\\nas implementing risk-aware behaviour. We begin\\nwith theoretical results in both the policy eval-\\nuation and control settings, exposing a signiﬁ-\\ncant distributional instability in the latter. We\\nthen use the distributional perspective to design\\na new algorithm which applies Bellman’s equa-\\ntion to the learning of approximate value distri-\\nbutions. We evaluate our algorithm using the\\nsuite of games from the Arcade Learning En-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='a new algorithm which applies Bellman’s equa-\\ntion to the learning of approximate value distri-\\nbutions. We evaluate our algorithm using the\\nsuite of games from the Arcade Learning En-\\nvironment. We obtain both state-of-the-art re-\\nsults and anecdotal evidence demonstrating the\\nimportance of the value distribution in approxi-\\nmate reinforcement learning. Finally, we com-\\nbine theoretical and empirical evidence to high-\\nlight the ways in which the value distribution im-\\npacts learning in the approximate setting.\\n1. Introduction\\nOne of the major tenets of reinforcement learning states\\nthat, when not otherwise constrained in its behaviour, an\\nagent should aim to maximize its expected utility Q, or\\nvalue (Sutton & Barto, 1998). Bellman’s equation succintly\\ndescribes this value in terms of the expected reward and ex-\\npected outcome of the random transition (x,a)→(X′,A′):\\nQ(x,a) =ER(x,a) +γEQ(X′,A′).\\nIn this paper, we aim to go beyond the notion of value and', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='pected outcome of the random transition (x,a)→(X′,A′):\\nQ(x,a) =ER(x,a) +γEQ(X′,A′).\\nIn this paper, we aim to go beyond the notion of value and\\nargue in favour of a distributional perspective on reinforce-\\n*Equal contribution1DeepMind, London, UK. Correspon-\\ndence to: Marc G. Bellemare <bellemare@google.com >.\\nProceedings of the 34thInternational Conference on Machine\\nLearning , Sydney, Australia, PMLR 70, 2017. Copyright 2017\\nby the author(s).ment learning. Speciﬁcally, the main object of our study is\\nthe random return Zwhose expectation is the value Q. This\\nrandom return is also described by a recursive equation, but\\none of a distributional nature:\\nZ(x,a)D=R(x,a) +γZ(X′,A′).\\nThedistributional Bellman equation states that the distribu-\\ntion ofZis characterized by the interaction of three random\\nvariables: the reward R, the next state-action (X′,A′), and\\nits random return Z(X′,A′). By analogy with the well-\\nknown case, we call this quantity the value distribution .', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='variables: the reward R, the next state-action (X′,A′), and\\nits random return Z(X′,A′). By analogy with the well-\\nknown case, we call this quantity the value distribution .\\nAlthough the distributional perspective is almost as old\\nas Bellman’s equation itself (Jaquette, 1973; Sobel, 1982;\\nWhite, 1988), in reinforcement learning it has thus far been\\nsubordinated to speciﬁc purposes: to model parametric un-\\ncertainty (Dearden et al., 1998), to design risk-sensitive al-\\ngorithms (Morimura et al., 2010b;a), or for theoretical anal-\\nysis (Azar et al., 2012; Lattimore & Hutter, 2012). By con-\\ntrast, we believe the value distribution has a central role to\\nplay in reinforcement learning.\\nContraction of the policy evaluation Bellman operator.\\nBasing ourselves on results by R ¨osler (1992) we show that,\\nfor a ﬁxed policy, the Bellman operator over value distribu-\\ntions is a contraction in a maximal form of the Wasserstein\\n(also called Kantorovich or Mallows) metric. Our partic-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='for a ﬁxed policy, the Bellman operator over value distribu-\\ntions is a contraction in a maximal form of the Wasserstein\\n(also called Kantorovich or Mallows) metric. Our partic-\\nular choice of metric matters: the same operator is not a\\ncontraction in total variation, Kullback-Leibler divergence,\\nor Kolmogorov distance.\\nInstability in the control setting. We will demonstrate an\\ninstability in the distributional version of Bellman’s opti-\\nmality equation, in contrast to the policy evaluation case.\\nSpeciﬁcally, although the optimality operator is a contrac-\\ntion in expected value (matching the usual optimality re-\\nsult), it is not a contraction in any metric over distributions.\\nThese results provide evidence in favour of learning algo-\\nrithms that model the effects of nonstationary policies.\\nBetter approximations. From an algorithmic standpoint,\\nthere are many beneﬁts to learning an approximate distribu-\\ntion rather than its approximate expectation. The distribu-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0})]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP 3 : Convert text chunks to vectors\n",
    "TEXT ------- > CHUNK ---------> Vectors ( Embeddings ) -------> vector Store (Chroma DB) / Faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\Documents\\Saranya\\GitHUb\\LANGCHAIN\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# ChromaDB vector database\n",
    "\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(documents[:5] , OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pected outcome of the random transition (x,a)→(X′,A′):\\nQ(x,a) =ER(x,a) +γEQ(X′,A′).\\nIn this paper, we aim to go beyond the notion of value and\\nargue in favour of a distributional perspective on reinforce-\\n*Equal contribution1DeepMind, London, UK. Correspon-\\ndence to: Marc G. Bellemare <bellemare@google.com >.\\nProceedings of the 34thInternational Conference on Machine\\nLearning , Sydney, Australia, PMLR 70, 2017. Copyright 2017\\nby the author(s).ment learning. Speciﬁcally, the main object of our study is\\nthe random return Zwhose expectation is the value Q. This\\nrandom return is also described by a recursive equation, but\\none of a distributional nature:\\nZ(x,a)D=R(x,a) +γZ(X′,A′).\\nThedistributional Bellman equation states that the distribu-\\ntion ofZis characterized by the interaction of three random\\nvariables: the reward R, the next state-action (X′,A′), and\\nits random return Z(X′,A′). By analogy with the well-\\nknown case, we call this quantity the value distribution .'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Query with normal query\n",
    "\n",
    "query =\"what is the paper about?\"\n",
    "result = db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faiss vector database\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(documents[:5] , OpenAIEmbeddings())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
