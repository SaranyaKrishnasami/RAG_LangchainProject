{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Retriever and chain - RAG pipeline\n",
    " What is chain and retreiver is done for advanced RAG\n",
    "\n",
    "What is chain ?\n",
    "\n",
    "What is Retreiver ?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(r\"C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='A Distributional Perspective on Reinforcement Learning\\nMarc G. Bellemare* 1Will Dabney* 1R´emi Munos1\\nAbstract\\nIn this paper we argue for the fundamental impor-\\ntance of the value distribution : the distribution\\nof the random return received by a reinforcement\\nlearning agent. This is in contrast to the com-\\nmon approach to reinforcement learning which\\nmodels the expectation of this return, or value .\\nAlthough there is an established body of liter-\\nature studying the value distribution, thus far it\\nhas always been used for a speciﬁc purpose such\\nas implementing risk-aware behaviour. We begin\\nwith theoretical results in both the policy eval-\\nuation and control settings, exposing a signiﬁ-\\ncant distributional instability in the latter. We\\nthen use the distributional perspective to design\\na new algorithm which applies Bellman’s equa-\\ntion to the learning of approximate value distri-\\nbutions. We evaluate our algorithm using the\\nsuite of games from the Arcade Learning En-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='vironment. We obtain both state-of-the-art re-\\nsults and anecdotal evidence demonstrating the\\nimportance of the value distribution in approxi-\\nmate reinforcement learning. Finally, we com-\\nbine theoretical and empirical evidence to high-\\nlight the ways in which the value distribution im-\\npacts learning in the approximate setting.\\n1. Introduction\\nOne of the major tenets of reinforcement learning states\\nthat, when not otherwise constrained in its behaviour, an\\nagent should aim to maximize its expected utility Q, or\\nvalue (Sutton & Barto, 1998). Bellman’s equation succintly\\ndescribes this value in terms of the expected reward and ex-\\npected outcome of the random transition (x,a)→(X′,A′):\\nQ(x,a) =ER(x,a) +γEQ(X′,A′).\\nIn this paper, we aim to go beyond the notion of value and\\nargue in favour of a distributional perspective on reinforce-\\n*Equal contribution1DeepMind, London, UK. Correspon-\\ndence to: Marc G. Bellemare <bellemare@google.com >.', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='Proceedings of the 34thInternational Conference on Machine\\nLearning , Sydney, Australia, PMLR 70, 2017. Copyright 2017\\nby the author(s).ment learning. Speciﬁcally, the main object of our study is\\nthe random return Zwhose expectation is the value Q. This\\nrandom return is also described by a recursive equation, but\\none of a distributional nature:\\nZ(x,a)D=R(x,a) +γZ(X′,A′).\\nThedistributional Bellman equation states that the distribu-\\ntion ofZis characterized by the interaction of three random\\nvariables: the reward R, the next state-action (X′,A′), and\\nits random return Z(X′,A′). By analogy with the well-\\nknown case, we call this quantity the value distribution .\\nAlthough the distributional perspective is almost as old\\nas Bellman’s equation itself (Jaquette, 1973; Sobel, 1982;\\nWhite, 1988), in reinforcement learning it has thus far been\\nsubordinated to speciﬁc purposes: to model parametric un-\\ncertainty (Dearden et al., 1998), to design risk-sensitive al-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='gorithms (Morimura et al., 2010b;a), or for theoretical anal-\\nysis (Azar et al., 2012; Lattimore & Hutter, 2012). By con-\\ntrast, we believe the value distribution has a central role to\\nplay in reinforcement learning.\\nContraction of the policy evaluation Bellman operator.\\nBasing ourselves on results by R ¨osler (1992) we show that,\\nfor a ﬁxed policy, the Bellman operator over value distribu-\\ntions is a contraction in a maximal form of the Wasserstein\\n(also called Kantorovich or Mallows) metric. Our partic-\\nular choice of metric matters: the same operator is not a\\ncontraction in total variation, Kullback-Leibler divergence,\\nor Kolmogorov distance.\\nInstability in the control setting. We will demonstrate an\\ninstability in the distributional version of Bellman’s opti-\\nmality equation, in contrast to the policy evaluation case.\\nSpeciﬁcally, although the optimality operator is a contrac-\\ntion in expected value (matching the usual optimality re-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='sult), it is not a contraction in any metric over distributions.\\nThese results provide evidence in favour of learning algo-\\nrithms that model the effects of nonstationary policies.\\nBetter approximations. From an algorithmic standpoint,\\nthere are many beneﬁts to learning an approximate distribu-\\ntion rather than its approximate expectation. The distribu-\\ntional Bellman operator preserves multimodality in value\\ndistributions, which we believe leads to more stable learn-\\ning. Approximating the full distribution also mitigates the\\neffects of learning from a nonstationary policy. As a whole,arXiv:1707.06887v1  [cs.LG]  21 Jul 2017', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 0}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nwe argue that this approach makes approximate reinforce-\\nment learning signiﬁcantly better behaved.\\nWe will illustrate the practical beneﬁts of the distributional\\nperspective in the context of the Arcade Learning Environ-\\nment (Bellemare et al., 2013). By modelling the value dis-\\ntribution within a DQN agent (Mnih et al., 2015), we ob-\\ntain considerably increased performance across the gamut\\nof benchmark Atari 2600 games, and in fact achieve state-\\nof-the-art performance on a number of games. Our results\\necho those of Veness et al. (2015), who obtained extremely\\nfast learning by predicting Monte Carlo returns.\\nFrom a supervised learning perspective, learning the full\\nvalue distribution might seem obvious: why restrict our-\\nselves to the mean? The main distinction, of course, is that\\nin our setting there are no given targets. Instead, we use\\nBellman’s equation to make the learning process tractable;', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 1}),\n",
       " Document(page_content='we must, as Sutton & Barto (1998) put it, “learn a guess\\nfrom a guess”. It is our belief that this guesswork ultimately\\ncarries more beneﬁts than costs.\\n2. Setting\\nWe consider an agent interacting with an environment in\\nthe standard fashion: at each step, the agent selects an ac-\\ntion based on its current state, to which the environment re-\\nsponds with a reward and the next state. We model this in-\\nteraction as a time-homogeneous Markov Decision Process\\n(X,A,R,P,γ ). As usual,XandAare respectively the\\nstate and action spaces, Pis the transition kernel P(·|x,a),\\nγ∈[0,1]is the discount factor, and Ris the reward func-\\ntion, which in this work we explicitly treat as a random\\nvariable. A stationary policy πmaps each state x∈X to a\\nprobability distribution over the action space A.\\n2.1. Bellman’s Equations\\nThereturnZπis the sum of discounted rewards along the\\nagent’s trajectory of interactions with the environment. The\\nvalue function Qπof a policyπdescribes the expected re-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 1}),\n",
       " Document(page_content='turn from taking action a∈ A from statex∈ X , then\\nacting according to π:\\nQπ(x,a) :=EZπ(x,a) =E[∞∑\\nt=0γtR(xt,at)]\\n,(1)\\nxt∼P(·|xt−1,at−1),at∼π(·|xt),x0=x,a0=a.\\nFundamental to reinforcement learning is the use of Bell-\\nman’s equation (Bellman, 1957) to describe the value func-\\ntion:\\nQπ(x,a) =ER(x,a) +γE\\nP,πQπ(x′,a′).\\nIn reinforcement learning we are typically interested in act-\\ning so as to maximize the return. The most common ap-\\n\\x00P⇡ZR+P⇡Z\\x00ZP⇡(a)(b)(c)(d)T⇡Z\\x00Figure 1. A distributional Bellman operator with a deterministic\\nreward function: (a) Next state distribution under policy π, (b)\\nDiscounting shrinks the distribution towards 0, (c) The reward\\nshifts it, and (d) Projection step (Section 4).\\nproach for doing so involves the optimality equation\\nQ∗(x,a) =ER(x,a) +γEPmax\\na′∈AQ∗(x′,a′).\\nThis equation has a unique ﬁxed point Q∗, the optimal\\nvalue function, corresponding to the set of optimal policies\\nΠ∗(π∗is optimal if Ea∼π∗Q∗(x,a) = maxaQ∗(x,a)).', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 1}),\n",
       " Document(page_content='We view value functions as vectors in RX×A, and the ex-\\npected reward function as one such vector. In this context,\\ntheBellman operatorTπandoptimality operator Tare\\nTπQ(x,a) :=ER(x,a) +γE\\nP,πQ(x′,a′) (2)\\nTQ(x,a) :=ER(x,a) +γEPmax\\na′∈AQ(x′,a′).(3)\\nThese operators are useful as they describe the expected\\nbehaviour of popular learning algorithms such as SARSA\\nand Q-Learning. In particular they are both contraction\\nmappings, and their repeated application to some initial Q0\\nconverges exponentially to QπorQ∗, respectively (Bert-\\nsekas & Tsitsiklis, 1996).\\n3. The Distributional Bellman Operators\\nIn this paper we take away the expectations inside Bell-\\nman’s equations and consider instead the full distribution\\nof the random variable Zπ. From here on, we will view Zπ\\nas a mapping from state-action pairs to distributions over\\nreturns, and call it the value distribution .\\nOur ﬁrst aim is to gain an understanding of the theoretical\\nbehaviour of the distributional analogues of the Bellman', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 1}),\n",
       " Document(page_content='operators, in particular in the less well-understood control\\nsetting. The reader strictly interested in the algorithmic\\ncontribution may choose to skip this section.\\n3.1. Distributional Equations\\nIt will sometimes be convenient to make use of the proba-\\nbility space (Ω,F,Pr). The reader unfamiliar with mea-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 1}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\nsure theory may think of Ωas the space of all possible\\noutcomes of an experiment (Billingsley, 1995). We will\\nwrite∥u∥pto denote the Lpnorm of a vector u∈RXfor\\n1≤p≤∞ ; the same applies to vectors in RX×A. The\\nLpnorm of a random vector U: Ω→RX(orRX×A) is\\nthen∥U∥p:=[\\nE[\\n∥U(ω)∥p\\np]]1/p, and forp=∞we have\\n∥U∥∞=esssup∥U(ω)∥∞(we will omit the dependency\\nonω∈Ωwhenever unambiguous). We will denote the\\nc.d.f. of a random variable UbyFU(y) := Pr{U≤y},\\nand its inverse c.d.f. by F−1\\nU(q) := inf{y:FU(y)≥q}.\\nA distributional equation UD:=Vindicates that the ran-\\ndom variable Uis distributed according to the same law\\nasV. Without loss of generality, the reader can understand\\nthe two sides of a distributional equation as relating the dis-\\ntributions of two independent random variables. Distribu-\\ntional equations have been used in reinforcement learning\\nby Engel et al. (2005); Morimura et al. (2010a) among oth-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 2}),\n",
       " Document(page_content='ers, and in operations research by White (1988).\\n3.2. The Wasserstein Metric\\nThe main tool for our analysis is the Wasserstein metric dp\\nbetween cumulative distribution functions (see e.g. Bickel\\n& Freedman, 1981, where it is called the Mallows metric).\\nForF,Gtwo c.d.fs over the reals, it is deﬁned as\\ndp(F,G) := inf\\nU,V∥U−V∥p,\\nwhere the inﬁmum is taken over all pairs of random vari-\\nables (U,V)with respective cumulative distributions F\\nandG. The inﬁmum is attained by the inverse c.d.f. trans-\\nform of a random variable Uuniformly distributed on [0,1]:\\ndp(F,G) =∥F−1(U)−G−1(U)∥p.\\nForp<∞this is more explicitly written as\\ndp(F,G) =(∫1\\n0⏐⏐F−1(u)−G−1(u)⏐⏐pdu)1/p\\n.\\nGiven two random variables U,Vwith c.d.fsFU,FV, we\\nwill writedp(U,V) :=dp(FU,FV). We will ﬁnd it conve-\\nnient to conﬂate the random variables under consideration\\nwith their versions under the inf, writing\\ndp(U,V) = inf\\nU,V∥U−V∥p.\\nwhenever unambiguous; we believe the greater legibility', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 2}),\n",
       " Document(page_content='justiﬁes the technical inaccuracy. Finally, we extend this\\nmetric to vectors of random variables, such as value distri-\\nbutions, using the corresponding Lpnorm.\\nConsider a scalar aand a random variable AindependentofU,V . The metric dphas the following properties:\\ndp(aU,aV )≤|a|dp(U,V) (P1)\\ndp(A+U,A +V)≤dp(U,V) (P2)\\ndp(AU,AV )≤∥A∥pdp(U,V). (P3)\\nWe will need the following additional property, which\\nmakes no independence assumptions on its variables. Its\\nproof, and that of later results, is given in the appendix.\\nLemma 1 (Partition lemma) .LetA1,A2,... be a set of\\nrandom variables describing a partition of Ω, i.e.Ai(ω)∈\\n{0,1}and for any ωthere is exactly one AiwithAi(ω) =\\n1. LetU,V be two random variables. Then\\ndp(\\nU,V)\\n≤∑\\nidp(AiU,AiV).\\nLetZdenote the space of value distributions with bounded\\nmoments. For two value distributions Z1,Z2∈Z we will\\nmake use of a maximal form of the Wasserstein metric:\\n¯dp(Z1,Z2) := sup\\nx,adp(Z1(x,a),Z2(x,a)).', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 2}),\n",
       " Document(page_content='We will use ¯dpto establish the convergence of the distribu-\\ntional Bellman operators.\\nLemma 2. ¯dpis a metric over value distributions.\\n3.3. Policy Evaluation\\nIn the policy evaluation setting (Sutton & Barto, 1998) we\\nare interested in the value function Vπassociated with a\\ngiven policy π. The analogue here is the value distribu-\\ntionZπ. In this section we characterize Zπand study the\\nbehaviour of the policy evaluation operator Tπ. We em-\\nphasize that Zπdescribes the intrinsic randomness of the\\nagent’s interactions with its environment, rather than some\\nmeasure of uncertainty about the environment itself.\\nWe view the reward function as a random vector R∈Z,\\nand deﬁne the transition operator Pπ:Z→Z\\nPπZ(x,a)D:=Z(X′,A′) (4)\\nX′∼P(·|x,a), A′∼π(·|X′),\\nwhere we use capital letters to emphasize the random na-\\nture of the next state-action pair (X′,A′). We deﬁne the\\ndistributional Bellman operator Tπ:Z→Z as\\nTπZ(x,a)D:=R(x,a) +γPπZ(x,a). (5)\\nWhileTπbears a surface resemblance to the usual Bell-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 2}),\n",
       " Document(page_content='man operator (2), it is fundamentally different. In particu-\\nlar, three sources of randomness deﬁne the compound dis-\\ntributionTπZ:', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 2}),\n",
       " Document(page_content='A Distributional Perspective on Reinforcement Learning\\na) The randomness in the reward R,\\nb) The randomness in the transition Pπ, and\\nc) The next-state value distribution Z(X′,A′).\\nIn particular, we make the usual assumption that these three\\nquantities are independent. In this section we will show\\nthat (5) is a contraction mapping whose unique ﬁxed point\\nis the random return Zπ.\\n3.3.1. C ONTRACTION IN ¯dp\\nConsider the process Zk+1:=TπZk, starting with some\\nZ0∈Z. We may expect the limiting expectation of {Zk}\\nto converge exponentially quickly, as usual, to Qπ. As we\\nnow show, the process converges in a stronger sense: Tπ\\nis a contraction in ¯dp, which implies that all moments also\\nconverge exponentially quickly.\\nLemma 3.Tπ:Z→Z is aγ-contraction in ¯dp.\\nUsing Lemma 3, we conclude using Banach’s ﬁxed point\\ntheorem thatTπhas a unique ﬁxed point. By inspection,\\nthis ﬁxed point must be Zπas deﬁned in (1). As we assume\\nall moments are bounded, this is sufﬁcient to conclude that', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 3}),\n",
       " Document(page_content='the sequence{Zk}converges to Zπin¯dpfor1≤p≤∞ .\\nTo conclude, we remark that not all distributional metrics\\nare equal; for example, Chung & Sobel (1987) have shown\\nthatTπis not a contraction in total variation distance. Sim-\\nilar results can be derived for the Kullback-Leibler diver-\\ngence and the Kolmogorov distance.\\n3.3.2. C ONTRACTION IN CENTERED MOMENTS\\nObserve that d2(U,V)(and more generally, dp) relates to a\\ncouplingC(ω) :=U(ω)−V(ω), in the sense that\\nd2\\n2(U,V)≤E[(U−V)2] =V(\\nC)\\n+(\\nEC)2.\\nAs a result, we cannot directly use d2to bound the variance\\ndifference|V(TπZ(x,a))−V(Zπ(x,a))|. However,Tπ\\nis in fact a contraction in variance (Sobel, 1982, see also\\nappendix). In general, Tπis not a contraction in the pth\\ncentered moment, p >2, but the centered moments of the\\niterates{Zk}still converge exponentially quickly to those\\nofZπ; the proof extends the result of R ¨osler (1992).\\n3.4. Control\\nThus far we have considered a ﬁxed policy π, and studied', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 3}),\n",
       " Document(page_content='the behaviour of its associated operator Tπ. We now set\\nout to understand the distributional operators of the control\\nsetting – where we seek a policy πthat maximizes value\\n– and the corresponding notion of an optimal value distri-\\nbution. As with the optimal value function, this notion is\\nintimately tied to that of an optimal policy. However, while\\nall optimal policies attain the same value Q∗, in our casea difﬁculty arises: in general there are many optimal value\\ndistributions.\\nIn this section we show that the distributional analogue\\nof the Bellman optimality operator converges, in a weak\\nsense, to the set of optimal value distributions. However,\\nthis operator is not a contraction in any metric between dis-\\ntributions, and is in general much more temperamental than\\nthe policy evaluation operators. We believe the conver-\\ngence issues we outline here are a symptom of the inherent\\ninstability of greedy updates, as highlighted by e.g. Tsitsik-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 3}),\n",
       " Document(page_content='lis (2002) and most recently Harutyunyan et al. (2016).\\nLetΠ∗be the set of optimal policies. We begin by charac-\\nterizing what we mean by an optimal value distribution .\\nDeﬁnition 1 (Optimal value distribution) .An optimal\\nvalue distribution is the v.d. of an optimal policy. The set\\nof optimal value distributions is Z∗:={Zπ∗:π∗∈Π∗}.\\nWe emphasize that not all value distributions with expecta-\\ntionQ∗are optimal: they must match the full distribution\\nof the return under some optimal policy.\\nDeﬁnition 2. A greedy policy πforZ∈Z maximizes the\\nexpectation of Z. The set of greedy policies for Zis\\nGZ:={π:∑\\naπ(a|x)EZ(x,a) = max\\na′∈AEZ(x,a′)}.\\nRecall that the expected Bellman optimality operator Tis\\nTQ(x,a) =ER(x,a) +γEPmax\\na′∈AQ(x′,a′).(6)\\nThe maximization at x′corresponds to some greedy policy.\\nAlthough this policy is implicit in (6), we cannot ignore it\\nin the distributional setting. We will call a distributional\\nBellman optimality operator any operatorTwhich imple-', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 3}),\n",
       " Document(page_content='ments a greedy selection rule, i.e.\\nTZ=TπZfor someπ∈GZ.\\nAs in the policy evaluation setting, we are interested in the\\nbehaviour of the iterates Zk+1:=TZk,Z0∈Z. Our ﬁrst\\nresult is to assert that EZkbehaves as expected.\\nLemma 4. LetZ1,Z2∈Z. Then\\n∥ETZ1−ETZ2∥∞≤γ∥EZ1−EZ2∥∞,\\nand in particular EZk→Q∗exponentially quickly.\\nBy inspecting Lemma 4, we might expect that Zkcon-\\nverges quickly in ¯dpto some ﬁxed point in Z∗. Unfor-\\ntunately, convergence is neither quick nor assured to reach\\na ﬁxed point. In fact, the best we can hope for is pointwise\\nconvergence, not even to the set Z∗but to the larger set of\\nnonstationary optimal value distributions .', metadata={'source': 'C:/Users/LENOVO/Documents/Saranya/GitHUb/LANGCHAIN/rag/DL_Paper.pdf', 'page': 3})]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000 , chunk_overlap = 20)\n",
    "text_splitter.split_documents(docs)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faiss vector database\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "db = FAISS.from_documents(docs[:20] , OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama\n",
    "llm =Ollama(model='llama2')\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will tip you $1000 if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create chain\n",
    "## using create stuff document chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "document_chain = create_stuff_documents_chain(llm , prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001A16C292E70>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Retrievers: A retriever is an interface that returns documents given\n",
    " an unstructured query. It is more general than a vector store.\n",
    " A retriever does not need to be able to store documents, only to \n",
    " return (or retrieve) them. Vector stores can be used as the backbone\n",
    " of a retriever, but there are other types of retrievers as well. \n",
    " https://python.langchain.com/docs/modules/data_connection/retrievers/   \n",
    "\"\"\"\n",
    "\n",
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine chain ( llm + prompt ) + Retreiver (faiss db as the retreiver )\n",
    "\n",
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"What is the Bellman’s equation\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Bellman's equation is a mathematical equation that describes the relationship between the value of a decision-making problem and the expected outcome of that decision. In reinforcement learning, Bellman's equation states that the value of a state-action pair is equal to the reward received plus the expected value of the next state-action pair, multiplied by a discount factor:\\n\\nQ(x,a) = ER(x,a) + γQ(X',A')\\n\\nwhere Q(x,a) is the value of the current state-action pair (x,a), ER(x,a) is the reward received, and γ is the discount factor. The equation states that the value of a decision is determined by the expected reward it receives plus the expected value of its next action.\\n\\nIn a broader sense, Bellman's equation can be applied to any decision-making problem, where the goal is to find the optimal policy that maximizes the expected value of the outcome. The equation can be used to model situations where there are multiple possible actions in a given state, and the goal is to determine the best course of action based on the expected outcome of each possible action.\\n\\nThe distributional Bellman equation extends the classical Bellman's equation by considering a distribution over the next state-action pair instead of a single value. This allows for modeling situations where the next state-action pair is not known in advance, but rather is determined randomly based on the current state and action:\\n\\nZ(x,a)D = R(x,a) + γZ(X',A')\\n\\nwhere Z(x,a)D is the distribution of the next state-action pair (X',A'), and R(x,a) is the reward received. The equation states that the distribution of the next state-action pair is determined by the reward received plus the expected distribution of the next action.\\n\\nThe distributional Bellman equation has important implications for reinforcement learning algorithms, as it allows for modeling situations where the agent's policy is not fixed but rather uncertain. This can lead to more realistic models of decision-making and better performance in uncertain environments.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
